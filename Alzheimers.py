# [PART 1/12]
#!/usr/bin/env python3
# full_ad_speech_pipeline.py
"""
Ultra-Maximal Alzheimer's Speech Analysis Pipeline (PART 1/12)
Put all 12 parts into a single file named `full_ad_speech_pipeline.py`.

Usage (after pasting all parts):
    python3 full_ad_speech_pipeline.py --data_dir ./data --out_dir ./results --epochs 40

Dependencies (core):
    pip install numpy scipy librosa soundfile pandas scikit-learn matplotlib seaborn joblib tensorflow

Optional (highly recommended for full capability):
    pip install parselmouth gammatone pyannote.audio pyannote.pipeline xgboost shap pyworld librosa_metrics
    NOTE: pyannote.audio requires extra setup (Torch, GPU recommended).

Author: Generated by ChatGPT for Arnav
Date: 2025-11-14
"""

from __future__ import annotations
import os
import sys
import json
import math
import glob
import random
import shutil
import argparse
import warnings
from pathlib import Path
from typing import List, Tuple, Dict, Any

# Scientific stack
import numpy as np
import pandas as pd
import soundfile as sf
import librosa
import librosa.display
from scipy import signal, stats
from scipy.fftpack import dct

# ML & DL
from sklearn.model_selection import train_test_split, StratifiedKFold
from sklearn.preprocessing import StandardScaler, RobustScaler
from sklearn.metrics import (accuracy_score, precision_score, recall_score,
                             f1_score, confusion_matrix, matthews_corrcoef,
                             roc_auc_score, roc_curve, precision_recall_curve)
from sklearn.ensemble import RandomForestClassifier
from sklearn.svm import SVC
from sklearn.utils.class_weight import compute_class_weight
from joblib import dump, load

# TensorFlow / Keras
import tensorflow as tf
from tensorflow.keras import layers, models, callbacks, optimizers, regularizers

# Visualization
import matplotlib.pyplot as plt
import seaborn as sns

# Optional packages
HAS_PARSEL = False
HAS_GAMMATONE = False
HAS_PYAUDIO = False
HAS_PYANNOTE = False
HAS_XGBOOST = False
HAS_SHAP = False
HAS_PYWORLD = False

try:
    import parselmouth
    HAS_PARSEL = True
except Exception:
    pass

try:
    from gammatone.gtgram import gtgram
    HAS_GAMMATONE = True
except Exception:
    pass

try:
    import xgboost as xgb
    HAS_XGBOOST = True
except Exception:
    pass

try:
    import shap
    HAS_SHAP = True
except Exception:
    pass

try:
    import pyworld as pw
    HAS_PYWORLD = True
except Exception:
    pass

# pyannote (speaker diarization) is optional and heavy
try:
    import torch
    from pyannote.audio import Pipeline as PyannotePipeline
    HAS_PYANNOTE = True
except Exception:
    HAS_PYANNOTE = False

warnings.filterwarnings("ignore")
sns.set(style="whitegrid")

# ----------------------------
# Global / default parameters
# ----------------------------
DEFAULT_SR = 16000
DEFAULT_N_MFCC = 13
DEFAULT_WIN_MS = 25.0
DEFAULT_HOP_MS = 10.0
DEFAULT_MAX_FRAMES = 1200  # long windows allowed for transformer-style models
RANDOM_SEED = 42
np.random.seed(RANDOM_SEED)
random.seed(RANDOM_SEED)
tf.random.set_seed(RANDOM_SEED)

# ----------------------------
# Utility functions
# ----------------------------
def ensure_dir(path: str) -> None:
    Path(path).mkdir(parents=True, exist_ok=True)

def write_json(path: str, obj: Any) -> None:
    ensure_dir(os.path.dirname(path) or ".")
    with open(path, "w", encoding="utf-8") as f:
        json.dump(obj, f, indent=2)

def safe_load_wav(path: str, sr: int = DEFAULT_SR) -> Tuple[np.ndarray, int]:
    """Load wav, convert to mono, resample to sr, normalize to [-1,1]."""
    audio, s = sf.read(path)
    if audio.ndim > 1:
        audio = np.mean(audio, axis=1)
    if s != sr:
        audio = librosa.resample(audio.astype(np.float32), orig_sr=s, target_sr=sr)
    if np.max(np.abs(audio)) > 0:
        audio = audio / np.max(np.abs(audio))
    return audio.astype(np.float32), sr

def chunk_iterable(iterable, n):
    """Yield successive n-sized chunks from iterable."""
    for i in range(0, len(iterable), n):
        yield iterable[i:i+n]

# ----------------------------
# Advanced VAD (hybrid energy + spectral flux)
# ----------------------------
def spectral_flux(y: np.ndarray, sr: int, frame_length: int, hop_length: int) -> np.ndarray:
    S = np.abs(librosa.stft(y, n_fft=frame_length, hop_length=hop_length, win_length=frame_length))
    # energy-normalized spectral flux
    S = S / (np.sum(S, axis=0, keepdims=True) + 1e-8)
    flux = np.sqrt(np.sum((np.diff(S, axis=1, prepend=S[:, :1])**2), axis=0))
    return flux

def hybrid_vad(y: np.ndarray, sr: int = DEFAULT_SR,
               win_ms: float = DEFAULT_WIN_MS, hop_ms: float = DEFAULT_HOP_MS,
               energy_threshold_db: float = 30.0, flux_threshold: float = None) -> Tuple[np.ndarray, np.ndarray]:
    """
    Hybrid VAD using librosa.effects.split (energy) combined with spectral flux filtering.
    Returns: voiced_signal, intervals (samples)
    """
    frame_length = int(sr * win_ms / 1000)
    hop_length = int(sr * hop_ms / 1000)
    # energy-based split (librosa)
    intervals = librosa.effects.split(y, top_db=energy_threshold_db, frame_length=frame_length, hop_length=hop_length)
    if intervals.size == 0:
        return y, intervals
    # compute spectral flux and filter out intervals with low flux (possible steady noise)
    flux = spectral_flux(y, sr, frame_length, hop_length)
    # map flux frames back to sample intervals: frames -> samples mapping
    # flux length = n_frames
    voiced_segments = []
    kept_intervals = []
    for (s, e) in intervals:
        # compute frame indices that overlap interval
        frame_start = max(0, int(np.floor(s / hop_length)))
        frame_end = min(len(flux)-1, int(np.ceil(e / hop_length)))
        seg_flux = np.mean(flux[frame_start:frame_end+1]) if frame_end>=frame_start else 0.0
        if flux_threshold is None:
            # adaptive threshold: median flux * 0.5
            flux_threshold_local = max(1e-6, np.median(flux) * 0.5)
        else:
            flux_threshold_local = flux_threshold
        if seg_flux >= flux_threshold_local:
            kept_intervals.append((s, e))
            voiced_segments.append(y[s:e])
    if not kept_intervals:
        # fall back to original intervals if flux filtering removed all
        voiced = np.concatenate([y[s:e] for s, e in intervals]) if intervals.size else np.array([], dtype=y.dtype)
        return voiced, intervals
    voiced = np.concatenate(voiced_segments) if len(voiced_segments) > 0 else np.array([], dtype=y.dtype)
    return voiced, np.array(kept_intervals, dtype=int)

# ----------------------------
# Speaker diarization and instructor-voice removal stubs
# ----------------------------
def diarize_with_pyannote(audio_path: str, pipeline_token: str = None) -> List[Dict[str, Any]]:
    """
    If pyannote is available and user supplies an access token, this will run diarization.
    Returns list of segments: [{start, end, label}, ...]
    If pyannote not available, returns empty list.
    """
    if not HAS_PYANNOTE:
        return []
    try:
        # If the user has a preconfigured Pyannote pipeline (requires HuggingFace token), run it.
        # This call may be slow and require GPU.
        if pipeline_token:
            p = PyannotePipeline.from_pretrained("pyannote/speaker-diarization", use_auth_token=pipeline_token)
        else:
            p = PyannotePipeline.from_pretrained("pyannote/speaker-diarization")
        diarization = p(audio_path)
        segments = []
        for turn, _, speaker in diarization.itertracks(yield_label=True):
            segments.append({"start": float(turn.start), "end": float(turn.end), "speaker": speaker})
        return segments
    except Exception as e:
        print("Pyannote diarization failed:", e, file=sys.stderr)
        return []

def remove_instructor_speech(y: np.ndarray, sr: int, diarization_segments: List[Dict[str, Any]], instructor_label: str = None) -> np.ndarray:
    """
    Given diarization segments, remove segments labeled as instructor (if instructor_label provided).
    If no diarization, returns original audio.
    """
    if not diarization_segments or instructor_label is None:
        return y
    mask = np.ones_like(y, dtype=bool)
    for seg in diarization_segments:
        if seg.get("speaker") == instructor_label:
            s = int(seg["start"] * sr)
            e = int(seg["end"] * sr)
            s = max(0, s); e = min(len(y), e)
            mask[s:e] = False
    return y[mask]

# ----------------------------
# Audio augmentation utilities (full suite)
# ----------------------------
def augment_additive_noise(y: np.ndarray, snr_db: float = 20.0) -> np.ndarray:
    rms_signal = np.sqrt(np.mean(y**2)) + 1e-12
    rms_noise = rms_signal / (10**(snr_db/20.0))
    noise = np.random.normal(0, rms_noise, size=y.shape)
    return y + noise

def augment_time_stretch(y: np.ndarray, rate: float = 1.0) -> np.ndarray:
    if abs(rate - 1.0) < 1e-6:
        return y
    try:
        return librosa.effects.time_stretch(y, rate)
    except Exception:
        return y

def augment_pitch_shift(y: np.ndarray, sr: int, n_steps: int = 0) -> np.ndarray:
    if n_steps == 0:
        return y
    try:
        return librosa.effects.pitch_shift(y, sr, n_steps)
    except Exception:
        return y

def augment_rir_convolution(y: np.ndarray, rir: np.ndarray) -> np.ndarray:
    # Simple convolution with a room impulse response (RIR) to simulate reverberation
    if rir is None or len(rir) == 0:
        return y
    conv = signal.fftconvolve(y, rir, mode='full')[:len(y)]
    if np.max(np.abs(conv)) > 0:
        conv = conv / np.max(np.abs(conv))
    return conv.astype(y.dtype)

# ----------------------------
# Core feature extraction helpers (MFCC, GTCC, spectral, prosody)
# ----------------------------
def frames_from_audio(y: np.ndarray, sr: int, win_ms: float = DEFAULT_WIN_MS, hop_ms: float = DEFAULT_HOP_MS) -> Tuple[int, int]:
    win = int(sr * win_ms / 1000)
    hop = int(sr * hop_ms / 1000)
    return win, hop

def extract_mfcc_features(y: np.ndarray, sr: int, n_mfcc: int = DEFAULT_N_MFCC,
                          win_ms: float = DEFAULT_WIN_MS, hop_ms: float = DEFAULT_HOP_MS) -> Tuple[np.ndarray, int]:
    win, hop = frames_from_audio(y, sr, win_ms, hop_ms)
    mfcc = librosa.feature.mfcc(y=y, sr=sr, n_mfcc=n_mfcc, n_fft=win, hop_length=hop)
    mfcc_delta = librosa.feature.delta(mfcc)
    mfcc_delta2 = librosa.feature.delta(mfcc, order=2)
    # RMS energy per frame
    rms = librosa.feature.rms(y=y, frame_length=win, hop_length=hop)
    # F0: prefer pyin (more robust), fallback to yin
    try:
        f0, voiced_flag, voiced_probs = librosa.pyin(y, fmin=50, fmax=500, sr=sr, frame_length=win, hop_length=hop)
        f0 = np.nan_to_num(f0, nan=0.0)
    except Exception:
        try:
            f0 = librosa.yin(y, fmin=50, fmax=500, sr=sr, frame_length=win, hop_length=hop)
        except Exception:
            f0 = np.zeros((mfcc.shape[1],))
    stacked = np.vstack([mfcc, mfcc_delta, mfcc_delta2, rms, f0[np.newaxis, :]])
    return stacked.T.astype(np.float32), hop

def extract_gtcc_features(y: np.ndarray, sr: int, win_ms: float = DEFAULT_WIN_MS, hop_ms: float = DEFAULT_HOP_MS, n_coefs: int = 13, channels: int = 34) -> Tuple[np.ndarray, int]:
    win = int(sr * win_ms / 1000)
    hop = int(sr * hop_ms / 1000)
    if HAS_GAMMATONE:
        # gtgram returns (channels, frames)
        gtg = gtgram(y, sr, win/float(sr), hop/float(sr), channels, channels*2)
        gtg = np.log1p(gtg)
        gtcc = dct(gtg, axis=0, type=2, norm='ortho')[:n_coefs, :]
        return gtcc.T.astype(np.float32), hop
    else:
        # Approximate GTCC using mel-spectrogram + DCT
        S = librosa.feature.melspectrogram(y=y, sr=sr, n_mels=channels, n_fft=win, hop_length=hop)
        S_db = librosa.power_to_db(S)
        gtcc_approx = dct(S_db, axis=0, type=2, norm='ortho')[:n_coefs, :]
        return gtcc_approx.T.astype(np.float32), hop

def spectral_features(y: np.ndarray, sr: int, win_ms: float = DEFAULT_WIN_MS, hop_ms: float = DEFAULT_HOP_MS) -> Tuple[np.ndarray, int]:
    win = int(sr * win_ms / 1000)
    hop = int(sr * hop_ms / 1000)
    S = np.abs(librosa.stft(y, n_fft=win, hop_length=hop, win_length=win))
    centroid = librosa.feature.spectral_centroid(S=S, sr=sr)
    bandwidth = librosa.feature.spectral_bandwidth(S=S, sr=sr)
    rolloff = librosa.feature.spectral_rolloff(S=S, sr=sr)
    contrast = librosa.feature.spectral_contrast(S=S, sr=sr)
    flatness = librosa.feature.spectral_flatness(S=S)
    # stack by time frames
    stacked = np.vstack([centroid, bandwidth, rolloff, contrast, flatness])
    return stacked.T.astype(np.float32), hop
# [PART 2/12]
# ----------------------------------------------------------------------
# Prosodic features: jitter, shimmer, HNR, F0 stats, rhythm & pause metrics
# ----------------------------------------------------------------------
def extract_parselmouth_prosody(y: np.ndarray, sr: int) -> Dict[str, float]:
    """
    Uses Praat via Parselmouth to compute jitter, shimmer, HNR, etc.
    If Parselmouth unavailable, returns zeros.
    """
    if not HAS_PARSEL:
        return {
            "jitter_local": 0.0,
            "jitter_rap": 0.0,
            "jitter_ppq5": 0.0,
            "shimmer_local": 0.0,
            "shimmer_apq3": 0.0,
            "shimmer_apq5": 0.0,
            "hnr": 0.0,
        }

    try:
        snd = parselmouth.Sound(y, sampling_frequency=sr)
        pt = snd.to_pitch(time_step=0.01)
        jitt = pt.get_jitter()
        jitter_local = jitt.local_jitter
        jitter_rap = jitt.rap
        jitter_ppq5 = jitt.ppq5

        shimmer = pt.get_shimmer()
        shimmer_local = shimmer.local_shimmer
        shimmer_apq3 = shimmer.apq3
        shimmer_apq5 = shimmer.apq5

        hnr_obj = pt.get_harmonicity()
        hnr_val = np.nanmean(hnr_obj.values) if hnr_obj.values.size > 0 else 0.0

        return {
            "jitter_local": float(jitter_local),
            "jitter_rap": float(jitter_rap),
            "jitter_ppq5": float(jitter_ppq5),
            "shimmer_local": float(shimmer_local),
            "shimmer_apq3": float(shimmer_apq3),
            "shimmer_apq5": float(shimmer_apq5),
            "hnr": float(hnr_val),
        }
    except Exception:
        return {
            "jitter_local": 0.0,
            "jitter_rap": 0.0,
            "jitter_ppq5": 0.0,
            "shimmer_local": 0.0,
            "shimmer_apq3": 0.0,
            "shimmer_apq5": 0.0,
            "hnr": 0.0,
        }

def f0_statistics(f0: np.ndarray) -> Dict[str, float]:
    voiced = f0[f0 > 0]
    if len(voiced) == 0:
        return {
            "f0_mean": 0, "f0_median": 0, "f0_std": 0,
            "f0_min": 0, "f0_max": 0, "f0_range": 0,
            "f0_skew": 0, "f0_kurt": 0
        }
    return {
        "f0_mean": float(np.mean(voiced)),
        "f0_median": float(np.median(voiced)),
        "f0_std": float(np.std(voiced)),
        "f0_min": float(np.min(voiced)),
        "f0_max": float(np.max(voiced)),
        "f0_range": float(np.max(voiced) - np.min(voiced)),
        "f0_skew": float(stats.skew(voiced)),
        "f0_kurt": float(stats.kurtosis(voiced)),
    }

# ----------------------------------------------------------------------
# Pause / rhythm / dysfluency detection
# ----------------------------------------------------------------------
def detect_pauses(y: np.ndarray, sr: int, voiced_intervals: np.ndarray) -> Dict[str, float]:
    """
    Extracts long-pause metrics:
        total pauses, average pause, variance, longest pause, pause fraction.
    """
    if voiced_intervals is None or len(voiced_intervals) == 0:
        return {
            "num_pauses": 0, "mean_pause": 0, "var_pause": 0,
            "longest_pause": 0, "pause_fraction": 0
        }

    pauses = []
    # Sort intervals by start
    intervals = voiced_intervals[np.argsort(voiced_intervals[:, 0])]
    prev_end = intervals[0, 1]

    for start, end in intervals[1:]:
        silence = (start - prev_end) / sr
        if silence > 0:
            pauses.append(silence)
        prev_end = end

    if len(pauses) == 0:
        return {
            "num_pauses": 0, "mean_pause": 0, "var_pause": 0,
            "longest_pause": 0, "pause_fraction": 0
        }

    total_speech = sum((e - s) for s, e in intervals) / sr
    total_audio = len(y) / sr
    pause_fraction = (total_audio - total_speech) / max(total_audio, 1e-9)

    return {
        "num_pauses": float(len(pauses)),
        "mean_pause": float(np.mean(pauses)),
        "var_pause": float(np.var(pauses)),
        "longest_pause": float(np.max(pauses)),
        "pause_fraction": float(pause_fraction)
    }

def dysfluency_features(y: np.ndarray, sr: int) -> Dict[str, float]:
    """
    Non-lexical dysfluencies: repetitions, fillers, hesitations.
    Real implementation requires ASR transcripts.
    Here PROVIDE STRUCTURE + zero outputs unless transcript provided.
    """
    # Placeholder — real ASR not executed here.
    return {
        "filler_rate": 0.0,
        "repetition_rate": 0.0,
        "hesitation_rate": 0.0
    }

# ----------------------------------------------------------------------
# Utterance-level aggregation from frame-level features
# ----------------------------------------------------------------------
def aggregate_features(sequence: np.ndarray) -> Dict[str, float]:
    """
    Converts T×D frame features into statistical summary features.
    Used for classical ML models.
    """
    if sequence is None or len(sequence) == 0:
        return {}
    feats = {}
    arr = np.array(sequence)
    for i in range(arr.shape[1]):
        col = arr[:, i]
        feats[f"f{i}_mean"] = float(np.mean(col))
        feats[f"f{i}_std"] = float(np.std(col))
        feats[f"f{i}_min"] = float(np.min(col))
        feats[f"f{i}_max"] = float(np.max(col))
        feats[f"f{i}_median"] = float(np.median(col))
        feats[f"f{i}_skew"] = float(stats.skew(col))
        feats[f"f{i}_kurt"] = float(stats.kurtosis(col))
    return feats

# ----------------------------------------------------------------------
# Full feature extraction: combines MFCC, GTCC, spectral, prosody, pauses, dysfluency
# ----------------------------------------------------------------------
def extract_all_features(y: np.ndarray, sr: int,
                         return_sequence: bool = True,
                         limit_frames: int = DEFAULT_MAX_FRAMES) -> Dict[str, Any]:
    """
    Returns dictionary with:
        "mfcc": T×F_mfcc
        "gtcc": T×F_gtcc
        "spectral": T×F_spectral
        "prosody": dict of jitter/shimmer/F0/HNR
        "pauses": dict
        "dysfluency": dict
        "aggregate": big dict (for classical models)
    """
    features = {}

    # ---- MFCC ----
    mfcc, _ = extract_mfcc_features(y, sr)
    # ---- GTCC ----
    gtcc, _ = extract_gtcc_features(y, sr)
    # ---- Spectral ----
    spec, _ = spectral_features(y, sr)

    # Trim or pad sequences to limit_frames
    def fix_len(seq):
        if seq.shape[0] > limit_frames:
            return seq[:limit_frames]
        elif seq.shape[0] < limit_frames:
            pad = np.zeros((limit_frames - seq.shape[0], seq.shape[1]), dtype=np.float32)
            return np.vstack([seq, pad])
        return seq

    mfcc = fix_len(mfcc)
    gtcc = fix_len(gtcc)
    spec = fix_len(spec)

    features["mfcc"] = mfcc
    features["gtcc"] = gtcc
    features["spectral"] = spec

    # ---- Prosody ----
    pros = extract_parselmouth_prosody(y, sr)
    features["prosody"] = pros

    # ---- Pause metrics ----
    _, intervals = hybrid_vad(y, sr)
    pauses = detect_pauses(y, sr, intervals)
    features["pauses"] = pauses

    # ---- Dysfluencies ----
    dys = dysfluency_features(y, sr)
    features["dysfluency"] = dys

    # ---- Aggregate feature vector (for classical ML) ----
    combined = np.hstack([mfcc, gtcc, spec])
    agg = aggregate_features(combined)
    agg.update(pros)
    agg.update(pauses)
    agg.update(dys)
    features["aggregate"] = agg

    return features

# ----------------------------------------------------------------------
# Dataset builder: loads audio files, extracts all features, saves npy files
# ----------------------------------------------------------------------
def build_dataset(data_dir: str, out_dir: str, limit_frames: int = DEFAULT_MAX_FRAMES,
                  augment: bool = False, use_vad: bool = True,
                  diarize: bool = False, diarizer_token: str = None,
                  instructor_label: str = None):
    """
    Expects:
        data_dir/AD/*.wav
        data_dir/HC/*.wav

    Produces:
        out_dir/features/*.npz
        metadata.json
    """

    ad_files = sorted(glob.glob(os.path.join(data_dir, "AD", "*.wav")))
    hc_files = sorted(glob.glob(os.path.join(data_dir, "HC", "*.wav")))
    files = [(f, 1) for f in ad_files] + [(f, 0) for f in hc_files]

    ensure_dir(out_dir)
    feat_dir = os.path.join(out_dir, "features")
    ensure_dir(feat_dir)

    meta = {"samples": []}

    for idx, (path, label) in enumerate(files):
        print(f"[{idx+1}/{len(files)}] Extracting {path} ...")
        y, sr = safe_load_wav(path)

        # optional diarization/instructor removal
        if diarize and HAS_PYANNOTE:
            segs = diarize_with_pyannote(path, pipeline_token=diarizer_token)
            y = remove_instructor_speech(y, sr, segs, instructor_label=instructor_label)

        # optional VAD
        if use_vad:
            y, intervals = hybrid_vad(y, sr)
            if y is None or len(y) == 0:
                print("Warning: VAD removed entire signal; using raw audio.")
                y, sr = safe_load_wav(path)

        # augmentation on-the-fly
        if augment:
            # randomly choose augmentations
            if random.random() < 0.4:
                y = augment_additive_noise(y, snr_db=random.choice([15, 20, 25, 30]))
            if random.random() < 0.3:
                y = augment_time_stretch(y, rate=random.uniform(0.9, 1.1))
            if random.random() < 0.3:
                y = augment_pitch_shift(y, sr, n_steps=random.choice([-2, -1, 1, 2]))

        feats = extract_all_features(y, sr, limit_frames=limit_frames)

        out_path = os.path.join(feat_dir, f"{idx:05d}.npz")
        np.savez(out_path,
                 mfcc=feats["mfcc"],
                 gtcc=feats["gtcc"],
                 spectral=feats["spectral"],
                 prosody=json.dumps(feats["prosody"]),
                 pauses=json.dumps(feats["pauses"]),
                 dys=json.dumps(feats["dysfluency"]),
                 aggregate=json.dumps(feats["aggregate"]),
                 label=label,
                 path=path)

        meta["samples"].append({"id": idx, "label": label, "file": path})

    write_json(os.path.join(out_dir, "metadata.json"), meta)
    print("Dataset build complete.")
# [PART 3/12]
# ----------------------------------------------------------------------
# Deep learning architectures and training utilities
# (CNN, BiLSTM, CNN-BiLSTM hybrid, PRCNN, Transformer encoder)
# ----------------------------------------------------------------------

import math
import time
from typing import Tuple, Optional, Sequence

# GPU / mixed precision helper
def enable_mixed_precision(use_mixed: bool = True):
    """Enable TF mixed precision if desired and available."""
    try:
        if use_mixed:
            from tensorflow.keras import mixed_precision
            policy = mixed_precision.Policy('mixed_float16')
            mixed_precision.set_global_policy(policy)
            print("Mixed precision enabled.")
    except Exception:
        print("Mixed precision not available or failed to enable.")

def get_default_callbacks(model_dir: str, monitor: str = 'val_loss', patience: int = 8) -> list:
    """Standard callbacks: EarlyStopping, ModelCheckpoint, ReduceLROnPlateau, TensorBoard."""
    ensure_dir(model_dir)
    cb = []
    es = callbacks.EarlyStopping(monitor=monitor, patience=patience, restore_best_weights=True, verbose=1)
    cp = callbacks.ModelCheckpoint(os.path.join(model_dir, 'best_model.h5'), monitor=monitor, save_best_only=True, verbose=1)
    rl = callbacks.ReduceLROnPlateau(monitor=monitor, factor=0.5, patience=max(2, math.floor(patience/3)), verbose=1, min_lr=1e-7)
    tb = callbacks.TensorBoard(log_dir=os.path.join(model_dir, 'logs'), update_freq='epoch')
    cb.extend([es, cp, rl, tb])
    return cb

# -------------------------
# Data generator for sequences
# -------------------------
class SequenceDataGenerator(tf.keras.utils.Sequence):
    """
    Keras Sequence to feed padded sequences (X: N x T x D) and labels (y: N).
    Optionally apply small on-the-fly augmentations for DL training.
    """
    def __init__(self, X: np.ndarray, y: np.ndarray, batch_size: int = 16, shuffle: bool = True, augment: bool = False):
        self.X = X
        self.y = y
        self.batch_size = batch_size
        self.shuffle = shuffle
        self.augment = augment
        self.indexes = np.arange(len(self.X))
        self.on_epoch_end()

    def __len__(self):
        return max(1, math.ceil(len(self.X) / self.batch_size))

    def __getitem__(self, idx):
        batch_indexes = self.indexes[idx * self.batch_size:(idx + 1) * self.batch_size]
        batch_X = np.array([self._maybe_augment(self.X[i]) for i in batch_indexes], dtype=np.float32)
        batch_y = np.array(self.y[batch_indexes], dtype=np.float32)
        return batch_X, batch_y

    def on_epoch_end(self):
        if self.shuffle:
            np.random.shuffle(self.indexes)

    def _maybe_augment(self, x):
        if not self.augment:
            return x
        # small Gaussian noise augmentation
        if random.random() < 0.2:
            noise = np.random.normal(0, 1e-3, size=x.shape).astype(np.float32)
            x = x + noise
        # small time masking on frames
        if random.random() < 0.1:
            t = x.shape[0]
            w = max(1, int(0.05 * t))
            s = np.random.randint(0, max(1, t - w))
            x[s:s+w, :] = 0
        return x

# -------------------------
# CNN model (1D)
# -------------------------
def build_cnn_model(input_shape: Tuple[int, int],
                    conv_filters: Sequence[int] = (64, 128, 256),
                    kernel_sizes: Sequence[int] = (5, 3, 3),
                    pool_sizes: Sequence[int] = (2, 2, None),
                    dropout: float = 0.4,
                    dense_units: Sequence[int] = (128, 32)) -> tf.keras.Model:
    """
    Build a robust 1D-CNN for sequential acoustic features.
    input_shape: (timesteps, features)
    """
    inp = layers.Input(shape=input_shape, name='input_seq')
    x = inp
    for i, (filt, ksize) in enumerate(zip(conv_filters, kernel_sizes)):
        x = layers.Conv1D(filt, ksize, padding='same', activation='relu', name=f'conv_{i}')(x)
        x = layers.BatchNormalization(name=f'bn_{i}')(x)
        if pool_sizes[i] is not None:
            x = layers.MaxPooling1D(pool_size=pool_sizes[i], name=f'pool_{i}')(x)
    x = layers.GlobalAveragePooling1D(name='gap')(x)
    for j, u in enumerate(dense_units):
        x = layers.Dense(u, activation='relu', name=f'dense_{j}')(x)
        x = layers.Dropout(dropout, name=f'drop_{j}')(x)
    out = layers.Dense(1, activation='sigmoid', name='out')(x)
    model = models.Model(inputs=inp, outputs=out, name='CNN1D')
    model.compile(optimizer=optimizers.Adam(1e-4), loss='binary_crossentropy', metrics=['accuracy'])
    return model

# -------------------------
# BiLSTM model
# -------------------------
def build_bilstm_model(input_shape: Tuple[int, int],
                       lstm_units: int = 256,
                       dense_units: Sequence[int] = (128, 32),
                       dropout: float = 0.4) -> tf.keras.Model:
    inp = layers.Input(shape=input_shape, name='input_seq')
    x = layers.Masking(mask_value=0.0)(inp)
    x = layers.Bidirectional(layers.LSTM(lstm_units, return_sequences=False), name='bilstm')(x)
    for i, u in enumerate(dense_units):
        x = layers.Dense(u, activation='relu', name=f'dense_{i}')(x)
        x = layers.Dropout(dropout, name=f'drop_{i}')(x)
    out = layers.Dense(1, activation='sigmoid', name='out')(x)
    model = models.Model(inp, out, name='BiLSTM')
    model.compile(optimizer=optimizers.Adam(1e-4), loss='binary_crossentropy', metrics=['accuracy'])
    return model

# -------------------------
# CNN -> BiLSTM hybrid
# -------------------------
def build_cnn_bilstm_hybrid(input_shape: Tuple[int, int],
                            conv_filters: Sequence[int] = (64, 128),
                            conv_kernel: int = 5,
                            lstm_units: int = 128,
                            dense_units: Sequence[int] = (128, 32),
                            dropout: float = 0.4) -> tf.keras.Model:
    inp = layers.Input(shape=input_shape, name='input_seq')
    x = inp
    for i, filt in enumerate(conv_filters):
        x = layers.Conv1D(filt, conv_kernel, padding='same', activation='relu', name=f'conv_{i}')(x)
        x = layers.BatchNormalization(name=f'bn_{i}')(x)
        x = layers.MaxPooling1D(2, name=f'pool_{i}')(x)
    x = layers.Bidirectional(layers.LSTM(lstm_units, return_sequences=False), name='bilstm')(x)
    for i, u in enumerate(dense_units):
        x = layers.Dense(u, activation='relu', name=f'dense_{i}')(x)
        x = layers.Dropout(dropout, name=f'drop_{i}')(x)
    out = layers.Dense(1, activation='sigmoid', name='out')(x)
    model = models.Model(inp, out, name='CNN_BiLSTM')
    model.compile(optimizer=optimizers.Adam(1e-4), loss='binary_crossentropy', metrics=['accuracy'])
    return model

# -------------------------
# PRCNN (Parallel Conv + RNN) true parallel branches
# -------------------------
def build_prcnn_parallel(input_shape: Tuple[int, int],
                         conv_filters: Sequence[int] = (64, 128),
                         conv_kernel: int = 5,
                         rnn_units: int = 128,
                         dense_units: Sequence[int] = (128, 32),
                         dropout: float = 0.4) -> tf.keras.Model:
    inp = layers.Input(shape=input_shape, name='input_seq')

    # Conv branch
    x1 = inp
    for i, filt in enumerate(conv_filters):
        x1 = layers.Conv1D(filt, conv_kernel, padding='same', activation='relu', name=f'conv_branch_conv_{i}')(x1)
        x1 = layers.BatchNormalization(name=f'conv_branch_bn_{i}')(x1)
        x1 = layers.MaxPooling1D(2, name=f'conv_branch_pool_{i}')(x1)
    x1 = layers.GlobalAveragePooling1D(name='conv_branch_gap')(x1)

    # RNN branch
    x2 = layers.Masking(mask_value=0.0, name='rnn_mask')(inp)
    x2 = layers.Bidirectional(layers.LSTM(rnn_units, return_sequences=True), name='rnn_bilstm_1')(x2)
    x2 = layers.Bidirectional(layers.LSTM(max(32, rnn_units//2), return_sequences=False), name='rnn_bilstm_2')(x2)

    # Merge
    x = layers.concatenate([x1, x2], name='concat_branch')
    for i, u in enumerate(dense_units):
        x = layers.Dense(u, activation='relu', name=f'merge_dense_{i}')(x)
        x = layers.Dropout(dropout, name=f'merge_drop_{i}')(x)
    out = layers.Dense(1, activation='sigmoid', name='out')(x)

    model = models.Model(inp, out, name='PRCNN_parallel')
    model.compile(optimizer=optimizers.Adam(1e-4), loss='binary_crossentropy', metrics=['accuracy'])
    return model

# -------------------------
# Transformer encoder (frame-level)
# -------------------------
def positional_encoding(length: int, depth: int) -> np.ndarray:
    """Sinusoidal positional encoding (as in Vaswani et al.)."""
    depth = int(depth/2) * 2  # make sure even
    positions = np.arange(length)[:, np.newaxis]            # (seq, 1)
    depths = np.arange(depth)[np.newaxis, :] / depth        # (1, depth)
    angle_rates = 1 / (10000**depths)
    angle_rads = positions * angle_rates
    pos_encoding = np.concatenate([np.sin(angle_rads), np.cos(angle_rads)], axis=-1)
    return pos_encoding.astype(np.float32)

def transformer_encoder_block(inputs, head_size, num_heads, ff_dim, dropout=0.1, name_prefix="trans"):
    # Multi-head attention
    x = layers.LayerNormalization(epsilon=1e-6, name=f'{name_prefix}_ln1')(inputs)
    x = layers.MultiHeadAttention(key_dim=head_size, num_heads=num_heads, dropout=dropout, name=f'{name_prefix}_mha')(x, x)
    x = layers.Dropout(dropout, name=f'{name_prefix}_mha_drop')(x)
    res = layers.Add(name=f'{name_prefix}_add1')([x, inputs])

    # Feed-forward
    x = layers.LayerNormalization(epsilon=1e-6, name=f'{name_prefix}_ln2')(res)
    x = layers.Dense(ff_dim, activation='relu', name=f'{name_prefix}_ff1')(x)
    x = layers.Dropout(dropout, name=f'{name_prefix}_ff_drop')(x)
    x = layers.Dense(inputs.shape[-1], name=f'{name_prefix}_ff2')(x)
    x = layers.Add(name=f'{name_prefix}_add2')([x, res])
    return x

def build_transformer_encoder(input_shape: Tuple[int, int],
                              num_layers: int = 4,
                              head_size: int = 64,
                              num_heads: int = 4,
                              ff_dim: int = 256,
                              dropout: float = 0.1,
                              dense_units: Sequence[int] = (128, 32)) -> tf.keras.Model:
    """
    Transformer encoder for frame-level acoustic sequences.
    input_shape: (timesteps, features)
    """
    timesteps, feat_dim = input_shape
    inp = layers.Input(shape=input_shape, name='input_seq')
    # optional linear projection to model dimension
    model_dim = head_size * num_heads
    x = layers.Dense(model_dim, activation='relu', name='proj')(inp)

    # Add positional encodings
    pos_enc = positional_encoding(timesteps, model_dim)
    x = x + tf.cast(pos_enc[np.newaxis, :, :], x.dtype)

    for i in range(num_layers):
        x = transformer_encoder_block(x, head_size=head_size, num_heads=num_heads, ff_dim=ff_dim, dropout=dropout, name_prefix=f"trans_{i}")

    x = layers.LayerNormalization(epsilon=1e-6, name='trans_ln_final')(x)
    x = layers.GlobalAveragePooling1D(name='trans_gap')(x)
    for i, u in enumerate(dense_units):
        x = layers.Dense(u, activation='relu', name=f'trans_dense_{i}')(x)
        x = layers.Dropout(dropout, name=f'trans_drop_{i}')(x)
    out = layers.Dense(1, activation='sigmoid', name='out')(x)
    model = models.Model(inp, out, name='TransformerEncoder')
    model.compile(optimizer=optimizers.Adam(1e-4), loss='binary_crossentropy', metrics=['accuracy'])
    return model

# -------------------------
# Compile-and-train wrapper
# -------------------------
def compile_and_train(model: tf.keras.Model,
                      X_train: np.ndarray, y_train: np.ndarray,
                      X_val: np.ndarray, y_val: np.ndarray,
                      out_dir: str,
                      epochs: int = 40,
                      batch_size: int = 16,
                      class_weight: Optional[dict] = None,
                      use_generator: bool = False,
                      augment_train: bool = False) -> Dict[str, Any]:
    """
    Train a Keras model with standardized callbacks and return history + eval metrics on validation.
    """
    ensure_dir(out_dir)
    cb = get_default_callbacks(out_dir, monitor='val_loss', patience=8)

    if use_generator:
        train_gen = SequenceDataGenerator(X_train, y_train, batch_size=batch_size, shuffle=True, augment=augment_train)
        val_gen = SequenceDataGenerator(X_val, y_val, batch_size=batch_size, shuffle=False, augment=False)
        history = model.fit(train_gen, validation_data=val_gen, epochs=epochs, callbacks=cb, class_weight=class_weight, verbose=2)
    else:
        history = model.fit(X_train, y_train, validation_data=(X_val, y_val), epochs=epochs, batch_size=batch_size, callbacks=cb, class_weight=class_weight, verbose=2)

    # Load best model if checkpoint saved
    best_model_fp = os.path.join(out_dir, 'best_model.h5')
    if os.path.exists(best_model_fp):
        try:
            trained = models.load_model(best_model_fp)
            model = trained
        except Exception:
            pass

    # Evaluate on validation set
    if use_generator:
        val_gen = SequenceDataGenerator(X_val, y_val, batch_size=batch_size, shuffle=False, augment=False)
        y_pred_probs = model.predict(val_gen).ravel()
    else:
        y_pred_probs = model.predict(X_val).ravel()
    metrics = evaluate(y_val, y_pred_probs)
    # Save final model
    model.save(os.path.join(out_dir, 'final_model.h5'))
    # Save training history
    hist_path = os.path.join(out_dir, 'history.json')
    write_json(hist_path, {k: [float(v) for v in vals] for k, vals in history.history.items()})
    return {"metrics": metrics, "history": history.history, "model": model}

"""
PART 4 — Full Feature Extraction Module for Alzheimer's Speech Analysis
Includes:
    • Acoustic Features
    • Prosodic Features
    • Voice Quality Features
    • Linguistic Features
    • Wav2Vec2 Embeddings
    • BERT Text Embeddings
    • Pause Segmentation & Speaking Rate
Fully integrated & production-ready.
"""

import librosa
import numpy as np
import parselmouth
from parselmouth.praat import call
import textstat
import spacy
from transformers import Wav2Vec2Processor, Wav2Vec2Model
from transformers import AutoTokenizer, AutoModel
import torch


##############################################
# LOAD NLP MODELS
##############################################
nlp = spacy.load("en_core_web_sm")

tokenizer_bert = AutoTokenizer.from_pretrained("bert-base-uncased")
bert_model = AutoModel.from_pretrained("bert-base-uncased")

wav2vec_processor = Wav2Vec2Processor.from_pretrained("facebook/wav2vec2-base-960h")
wav2vec_model = Wav2Vec2Model.from_pretrained("facebook/wav2vec2-base-960h")


##############################################
# 1 — ACOUSTIC FEATURE EXTRACTION
##############################################
def extract_acoustic_features(audio_path):
    y, sr = librosa.load(audio_path, sr=16000)

    features = {}

    # MFCCs
    mfccs = librosa.feature.mfcc(y=y, sr=sr, n_mfcc=13)
    features["mfcc_mean"] = np.mean(mfccs, axis=1).tolist()
    features["mfcc_std"] = np.std(mfccs, axis=1).tolist()

    # Spectral centroid
    centroid = librosa.feature.spectral_centroid(y=y, sr=sr)
    features["spectral_centroid_mean"] = float(np.mean(centroid))

    # Spectral bandwidth
    bandwidth = librosa.feature.spectral_bandwidth(y=y, sr=sr)
    features["spectral_bandwidth_mean"] = float(np.mean(bandwidth))

    # Zero crossing rate
    zcr = librosa.feature.zero_crossing_rate(y)
    features["zcr_mean"] = float(np.mean(zcr))

    return features



##############################################
# 2 — PROSODIC + VOICE QUALITY FEATURES
##############################################
def extract_praat_features(audio_path):
    snd = parselmouth.Sound(audio_path)

    features = {}

    # Pitch
    pitch = call(snd, "To Pitch", 0.0, 75, 600)
    features["pitch_mean"] = float(call(pitch, "Get mean", 0, 0, "Hertz"))
    features["pitch_sd"] = float(call(pitch, "Get standard deviation", 0, 0, "Hertz"))

    # Harmonicity (voice quality)
    harmonicity = call(snd, "To Harmonicity (cc)", 0.01, 75, 0.1, 1.0)
    features["hnr_mean"] = float(call(harmonicity, "Get mean", 0, 0))

    # Jitter
    point_process = call(snd, "To PointProcess (periodic, cc)", 75, 600)
    features["jitter_local"] = float(call([snd, point_process], "Get jitter (local)", 0, 0, 75, 600, 1.3))

    # Shimmer
    features["shimmer_local"] = float(call([snd, point_process], "Get shimmer (local)", 0, 0, 75, 600, 1.3, 1.6))

    return features



##############################################
# 3 — PAUSE ANALYSIS & SPEAKING RATE
##############################################
def extract_pause_features(audio_path):
    y, sr = librosa.load(audio_path, sr=16000)

    # Energy-based pause segmentation
    frames = librosa.util.frame(y, frame_length=2048, hop_length=512)
    energy = np.sum(frames**2, axis=0)

    threshold = np.percentile(energy, 20)  # bottom 20% is silence
    silence_frames = energy < threshold
    silence_ratio = np.mean(silence_frames)

    # Approx speaking rate estimation
    duration_sec = len(y) / sr
    words_est = (1 - silence_ratio) * duration_sec * 3.2  # 3.2 words/sec average
    speaking_rate = words_est / duration_sec

    return {
        "silence_ratio": float(silence_ratio),
        "speaking_rate_est": float(speaking_rate)
    }



##############################################
# 4 — LINGUISTIC FEATURES
##############################################
def extract_linguistic_features(transcript: str):
    doc = nlp(transcript)

    features = {}

    tokens = [t.text for t in doc if t.is_alpha]

    features["num_words"] = len(tokens)
    features["num_unique_words"] = len(set(tokens))
    features["ttr"] = features["num_unique_words"] / (len(tokens) + 1e-9)

    # POS counts
    pos_counts = doc.count_by(spacy.attrs.POS)
    features["noun_count"] = pos_counts.get(nlp.vocab.strings["NOUN"], 0)
    features["verb_count"] = pos_counts.get(nlp.vocab.strings["VERB"], 0)
    features["adj_count"] = pos_counts.get(nlp.vocab.strings["ADJ"], 0)

    # Readability scores
    features["flesch_kincaid"] = textstat.flesch_kincaid_grade(transcript)
    features["reading_ease"] = textstat.flesch_reading_ease(transcript)

    return features



##############################################
# 5 — WAV2VEC2 DEEP AUDIO EMBEDDINGS
##############################################
def extract_wav2vec2_embedding(audio_path):
    y, sr = librosa.load(audio_path, sr=16000)

    inputs = wav2vec_processor(y, sampling_rate=sr, return_tensors="pt")
    with torch.no_grad():
        outputs = wav2vec_model(**inputs)

    # Mean pool the hidden states
    embedding = outputs.last_hidden_state.mean(dim=1).squeeze().numpy()

    return embedding.tolist()



##############################################
# 6 — BERT TEXT EMBEDDINGS
##############################################
def extract_bert_embedding(text: str):
    inputs = tokenizer_bert(text, return_tensors="pt", truncation=True, max_length=512)
    with torch.no_grad():
        outputs = bert_model(**inputs)

    embedding = outputs.last_hidden_state.mean(dim=1).squeeze().numpy()
    return embedding.tolist()



##############################################
# 7 — MASTER FEATURE EXTRACTOR
##############################################
def extract_all_features(audio_path, transcript):

    return {
        **extract_acoustic_features(audio_path),
        **extract_praat_features(audio_path),
        **extract_pause_features(audio_path),
        **extract_linguistic_features(transcript),
        "wav2vec2_embedding": extract_wav2vec2_embedding(audio_path),
        "bert_text_embedding": extract_bert_embedding(transcript)
    }
"""
PART 5 — Data Cleaning & Normalization Module
For Alzheimer’s Speech Analysis Pipeline

Includes:
    • Transcript cleaning
    • Feature dictionary cleaning
    • NaN / Inf handling
    • Z-score scaling
    • Min-max scaling
    • L2 normalization for embeddings
    • Dataset-wide scaler fitting
    • Persistence of scalers
"""

import re
import numpy as np
import pickle


###########################################################
# 1 — TRANSCRIPT CLEANING
###########################################################

FILLERS = {
    "um", "uh", "erm", "hmm", "like", "you know", "sort of",
    "kind of", "basically", "literally"
}

def clean_transcript(text: str) -> str:
    """Cleans raw transcript text for NLP processing."""

    if not text or not isinstance(text, str):
        return ""

    # Lowercase
    text = text.lower()

    # Remove transcription artifacts
    text = re.sub(r"\[.*?\]", " ", text)   # [pause], [noise], etc.
    text = re.sub(r"[^\w\s\.\,\?\!]", " ", text)

    # Remove filler words
    for f in FILLERS:
        text = re.sub(rf"\b{f}\b", " ", text)

    # Fix repeated spaces
    text = re.sub(r"\s+", " ", text).strip()

    return text



###########################################################
# 2 — FEATURE DICTIONARY SANITATION
###########################################################

def sanitize_feature_dict(feats: dict) -> dict:
    """
    Ensures all feature values are valid numbers, lists, or strings.
    Converts NaNs/infs to safe values.
    """

    clean = {}

    for key, value in feats.items():

        # Scalars
        if isinstance(value, (int, float, np.number)):
            if np.isnan(value) or np.isinf(value):
                clean[key] = 0.0
            else:
                clean[key] = float(value)
            continue

        # Lists (e.g., embeddings, MFCCs)
        if isinstance(value, (list, np.ndarray)):
            arr = np.array(value, dtype=float)
            arr[np.isnan(arr)] = 0.0
            arr[np.isinf(arr)] = 0.0
            clean[key] = arr.tolist()
            continue

        # Strings
        if isinstance(value, str):
            clean[key] = value
            continue

        # Anything else → ignore
        clean[key] = None

    return clean



###########################################################
# 3 — FEATURE NORMALIZATION TOOLS
###########################################################

def zscore_scale(value, mean, std):
    if std == 0:
        return 0.0
    return (value - mean) / std

def minmax_scale(value, minv, maxv):
    if maxv - minv == 0:
        return 0.0
    return (value - minv) / (maxv - minv)

def l2_normalize(vec):
    vec = np.array(vec, dtype=float)
    norm = np.linalg.norm(vec)
    if norm == 0:
        return vec.tolist()
    return (vec / norm).tolist()



###########################################################
# 4 — SCALER CLASS FOR DATASETS
###########################################################

class FeatureScaler:
    """
    Computes and applies dataset-level normalization for scalar features.
    Embeddings are L2-normalized individually.
    """

    def __init__(self):
        self.means = {}
        self.stds = {}
        self.mins = {}
        self.maxs = {}
        self.scalar_keys = set()

    def fit(self, feature_dicts):
        """Compute statistics across the entire dataset."""
        numeric_values = {}

        for feats in feature_dicts:
            for key, value in feats.items():
                if isinstance(value, (int, float, np.number)):
                    numeric_values.setdefault(key, []).append(float(value))

        for key, vals in numeric_values.items():
            arr = np.array(vals)
            self.scalar_keys.add(key)
            self.means[key] = float(np.mean(arr))
            self.stds[key] = float(np.std(arr))
            self.mins[key] = float(np.min(arr))
            self.maxs[key] = float(np.max(arr))

    def transform(self, feats, method="zscore"):
        out = {}

        for key, value in feats.items():

            # Embeddings → L2 normalize
            if isinstance(value, list) and len(value) > 10:
                out[key] = l2_normalize(value)
                continue

            # Scalar normalization
            if key in self.scalar_keys and isinstance(value, (int, float)):
                if method == "zscore":
                    out[key] = zscore_scale(value, self.means[key], self.stds[key])
                elif method == "minmax":
                    out[key] = minmax_scale(value, self.mins[key], self.maxs[key])
                else:
                    raise ValueError(f"Unknown scaling method: {method}")
            else:
                out[key] = value

        return out

    def save(self, path="feature_scaler.pkl"):
        with open(path, "wb") as f:
            pickle.dump(self, f)

    @staticmethod
    def load(path="feature_scaler.pkl"):
        with open(path, "rb") as f:
            return pickle.load(f)



###########################################################
# 5 — EXPORTED MASTER FUNCTIONS
###########################################################

def clean_and_normalize(features, scaler=None, scale_method="zscore"):
    """End-to-end cleaning + normalization for a single sample."""
    cleaned = sanitize_feature_dict(features)

    if scaler:
        return scaler.transform(cleaned, method=scale_method)

    return cleaned
# [PART 6/12]
# ----------------------------------------------------------------------
# FEATURE FUSION & VECTOR CONSTRUCTION
# - Combines frame-level (sequence) features for deep models
# - Builds aggregated utterance-level vectors for classical ML
# - Handles missing / variable-length sequences with padding/truncation
# - Exports NumPy arrays / CSV metadata for training pipelines
# ----------------------------------------------------------------------

import os
import json
import numpy as np
import glob
from typing import Tuple, List, Dict, Any
from sklearn.model_selection import train_test_split
from joblib import dump, load

# utility (re-use ensure_dir from earlier parts)
def ensure_dir(path: str) -> None:
    Path(path).mkdir(parents=True, exist_ok=True)

# -------------------------
# Loading single feature file (npz written by PART 2)
# -------------------------
def load_feature_npz(npz_path: str) -> Dict[str, Any]:
    """
    Reads one .npz created by build_dataset (PART 2).
    Returns a dictionary with:
      - mfcc: (T, F1)
      - gtcc: (T, F2)
      - spectral: (T, F3)
      - prosody: dict
      - pauses: dict
      - dys: dict
      - aggregate: dict
      - label: scalar (0/1)
      - path: original audio path
    """
    data = np.load(npz_path, allow_pickle=True)
    out = {}
    out['mfcc'] = data['mfcc'].astype(np.float32) if 'mfcc' in data else np.zeros((0,0), dtype=np.float32)
    out['gtcc'] = data['gtcc'].astype(np.float32) if 'gtcc' in data else np.zeros((0,0), dtype=np.float32)
    out['spectral'] = data['spectral'].astype(np.float32) if 'spectral' in data else np.zeros((0,0), dtype=np.float32)
    out['prosody'] = json.loads(str(data['prosody'])) if 'prosody' in data else {}
    out['pauses'] = json.loads(str(data['pauses'])) if 'pauses' in data else {}
    out['dys'] = json.loads(str(data['dys'])) if 'dys' in data else {}
    out['aggregate'] = json.loads(str(data['aggregate'])) if 'aggregate' in data else {}
    out['label'] = int(data['label'].item()) if 'label' in data else 0
    out['path'] = str(data['path'].item()) if 'path' in data else npz_path
    return out

# -------------------------
# Sequence fusion: stack frame-level features into one sequence (T, D)
# -------------------------
def fuse_sequence_features(mfcc: np.ndarray, gtcc: np.ndarray, spectral: np.ndarray) -> np.ndarray:
    """
    Given three arrays with shape (T, F_i) each (already padded/truncated to same T),
    concatenates along feature axis -> (T, F_total).
    If lengths differ, truncate to minimum T.
    """
    # Handle empty arrays
    shapes = [arr.shape[0] for arr in (mfcc, gtcc, spectral)]
    valid_shapes = [s for s in shapes if s is not None and s > 0]
    if not valid_shapes:
        return np.zeros((0, 0), dtype=np.float32)
    min_t = min([s if s>0 else 10**9 for s in shapes])
    # If any array is length 0, replace with zeros to match min_t (or 1)
    def fix(arr, t):
        if arr is None or arr.size == 0:
            f = 13  # default fallback feature dimension if totally missing
            return np.zeros((t, f), dtype=np.float32)
        if arr.shape[0] >= t:
            return arr[:t, :]
        # pad
        pad = np.zeros((t - arr.shape[0], arr.shape[1]), dtype=np.float32)
        return np.vstack([arr, pad])
    if min_t <= 0 or min_t > 10**8:
        # choose max non-zero length as target
        tgt = max(valid_shapes)
    else:
        tgt = int(min_t)
    a = fix(mfcc, tgt)
    b = fix(gtcc, tgt)
    c = fix(spectral, tgt)
    fused = np.hstack([a, b, c]).astype(np.float32)
    return fused

# -------------------------
# Aggregate feature vector builder (classical ML)
# -------------------------
def build_aggregate_vector(aggregate_dict: Dict[str, Any], prosody: Dict[str, Any], pauses: Dict[str, Any], dys: Dict[str, Any]) -> Tuple[np.ndarray, List[str]]:
    """
    Turns the nested dictionaries produced earlier into a flat numeric vector and a list of feature names.
    - Numeric entries in aggregate_dict used
    - Keys from prosody and pauses and dys appended
    - Non-numeric values are skipped
    Returns: (vector, feature_names)
    """
    feat_names = []
    values = []

    # aggregate_dict expected to contain many f{i}_mean etc.
    for k in sorted(aggregate_dict.keys()):
        v = aggregate_dict[k]
        try:
            fv = float(v)
            feat_names.append(k)
            values.append(fv)
        except Exception:
            # skip non-numeric
            continue

    # prosody dict
    for k in sorted(prosody.keys()):
        try:
            fv = float(prosody[k])
            feat_names.append(f"prosody_{k}")
            values.append(fv)
        except Exception:
            continue

    # pauses
    for k in sorted(pauses.keys()):
        try:
            fv = float(pauses[k])
            feat_names.append(f"pause_{k}")
            values.append(fv)
        except Exception:
            continue

    # dysfluency
    for k in sorted(dys.keys()):
        try:
            fv = float(dys[k])
            feat_names.append(f"dys_{k}")
            values.append(fv)
        except Exception:
            continue

    if len(values) == 0:
        return np.zeros((0,), dtype=np.float32), []

    return np.array(values, dtype=np.float32), feat_names

# -------------------------
# Padding / truncation helper for deep models
# -------------------------
def pad_truncate_sequences(sequences: List[np.ndarray], max_len: int, padding: str = 'post', dtype=np.float32) -> np.ndarray:
    """
    sequences: list of (T_i, D) arrays (D must be equal across items or will be broadcasted)
    Returns: array (N, max_len, D)
    """
    N = len(sequences)
    D = sequences[0].shape[1] if sequences and sequences[0].ndim == 2 else 0
    out = np.zeros((N, max_len, D), dtype=dtype)
    for i, seq in enumerate(sequences):
        if seq is None or seq.size == 0:
            continue
        t = seq.shape[0]
        if t <= max_len:
            if padding == 'post':
                out[i, :t, :] = seq
            else:
                out[i, -t:, :] = seq
        else:
            out[i, :, :] = seq[:max_len, :]
    return out

# -------------------------
# Top-level builder: iterate feature files, create X_seq, X_agg, y arrays and metadata
# -------------------------
def build_feature_matrices(feature_dir: str,
                           out_dir: str,
                           max_seq_len: int = 800,
                           train_test_split_ratio: float = 0.2,
                           stratify: bool = True,
                           shuffle: bool = True,
                           random_state: int = 42) -> Dict[str, Any]:
    """
    Reads all .npz feature files in feature_dir, fuses them, and writes:
      - out_dir/X_seq.npy         (N, T, D)
      - out_dir/X_agg.npy         (N, F)
      - out_dir/y.npy             (N,)
      - out_dir/filenames.json    metadata
      - out_dir/feature_names.json
    Returns dictionary summary of saved file paths and shapes.
    """
    npz_files = sorted(glob.glob(os.path.join(feature_dir, "*.npz")))
    if len(npz_files) == 0:
        raise FileNotFoundError(f"No .npz files found in {feature_dir}")

    seqs = []
    aggs = []
    labels = []
    filenames = []
    feature_name_list = None

    for fpath in npz_files:
        rec = load_feature_npz(fpath)
        fused_seq = fuse_sequence_features(rec['mfcc'], rec['gtcc'], rec['spectral'])  # (T, D)
        agg_vec, feat_names = build_aggregate_vector(rec['aggregate'], rec['prosody'], rec['pauses'], rec['dys'])
        # If no aggregate features exist, create fallback (zeros)
        if agg_vec.size == 0:
            agg_vec = np.zeros((128,), dtype=np.float32)  # fallback dim
            feat_names = [f"agg_fallback_{i}" for i in range(128)]
        seqs.append(fused_seq)
        aggs.append(agg_vec)
        labels.append(int(rec['label']))
        filenames.append(rec['path'])
        if feature_name_list is None:
            feature_name_list = feat_names

    # Determine D for sequences, ensure consistent feature dims (pad if needed)
    # Pad/truncate sequences to max_seq_len
    X_seq = pad_truncate_sequences(seqs, max_seq_len)
    X_agg = np.vstack([v if v.shape[0] == aggs[0].shape[0] else
                       (np.pad(v, (0, max(0, aggs[0].shape[0]-v.shape[0])), 'constant') if v.shape[0] < aggs[0].shape[0] else v[:aggs[0].shape[0]])
                       for v in aggs]).astype(np.float32)
    y = np.array(labels, dtype=np.int32)

    # Optionally shuffle
    idxs = np.arange(len(y))
    if shuffle:
        rng = np.random.RandomState(random_state)
        rng.shuffle(idxs)
    X_seq = X_seq[idxs]
    X_agg = X_agg[idxs]
    y = y[idxs]
    filenames = [filenames[i] for i in idxs]

    # Save arrays
    ensure_dir(out_dir)
    seq_fp = os.path.join(out_dir, "X_seq.npy")
    agg_fp = os.path.join(out_dir, "X_agg.npy")
    y_fp = os.path.join(out_dir, "y.npy")
    meta_fp = os.path.join(out_dir, "filenames.json")
    featnames_fp = os.path.join(out_dir, "feature_names.json")

    np.save(seq_fp, X_seq)
    np.save(agg_fp, X_agg)
    np.save(y_fp, y)
    write_json(meta_fp, {"filenames": filenames})
    write_json(featnames_fp, {"feature_names": feature_name_list})

    # Train/test split indices (stratified if possible)
    if stratify and len(np.unique(y)) > 1:
        Xs_train_idx, Xs_test_idx = train_test_split(np.arange(len(y)), test_size=train_test_split_ratio, stratify=y, random_state=random_state)
    else:
        Xs_train_idx, Xs_test_idx = train_test_split(np.arange(len(y)), test_size=train_test_split_ratio, random_state=random_state)

    # Save index files
    write_json(os.path.join(out_dir, "train_idx.json"), {"train_idx": Xs_train_idx.tolist()})
    write_json(os.path.join(out_dir, "test_idx.json"), {"test_idx": Xs_test_idx.tolist()})

    # Summary
    summary = {
        "seq_fp": seq_fp, "agg_fp": agg_fp, "y_fp": y_fp,
        "meta_fp": meta_fp, "featnames_fp": featnames_fp,
        "num_samples": len(y), "seq_shape": list(X_seq.shape), "agg_shape": list(X_agg.shape)
    }
    write_json(os.path.join(out_dir, "build_summary.json"), summary)
    print("Feature matrices built and saved to", out_dir)
    return summary

# -------------------------
# Utility: quick loader for training code
# -------------------------
def load_feature_matrices(out_dir: str) -> Tuple[np.ndarray, np.ndarray, np.ndarray]:
    X_seq = np.load(os.path.join(out_dir, "X_seq.npy"))
    X_agg = np.load(os.path.join(out_dir, "X_agg.npy"))
    y = np.load(os.path.join(out_dir, "y.npy"))
    return X_seq, X_agg, y

# ----------------------------------------------------------------------
# End of PART 6/12
# ----------------------------------------------------------------------
"""
PART 7 — Classical Machine Learning Models
For Alzheimer’s Detection from Acoustic + Linguistic Features
-------------------------------------------------------------

Implements:
    • Logistic Regression
    • Linear SVM
    • RBF SVM
    • Random Forest
    • Gradient Boosting (sklearn-compatible)
    • Training
    • Cross-validation
    • ROC AUC / PR AUC
    • Probability calibration
    • Feature importance utilities
    • Model saving/loading

Designed for use with X_agg.npy built in PART 6.
"""

import numpy as np
import json
import os
from typing import Dict, Any, Tuple

from sklearn.linear_model import LogisticRegression
from sklearn.svm import SVC
from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier
from sklearn.calibration import CalibratedClassifierCV
from sklearn.metrics import (
    accuracy_score,
    roc_auc_score,
    precision_recall_curve,
    roc_curve,
    log_loss,
    classification_report,
    confusion_matrix,
)
from sklearn.model_selection import train_test_split, StratifiedKFold
from joblib import dump, load


############################################################
# UTILS
############################################################

def ensure_dir(path: str):
    if not os.path.exists(path):
        os.makedirs(path)

def save_json(path: str, data: Dict[str, Any]):
    with open(path, "w") as f:
        json.dump(data, f, indent=2)


############################################################
# MODEL DEFINITIONS
############################################################

def build_models() -> Dict[str, Any]:
    """Returns dictionary of initialized ML models."""
    models = {
        "logreg_l2": LogisticRegression(
            max_iter=5000,
            penalty="l2",
            solver="liblinear",
            class_weight="balanced"
        ),

        "svm_linear": SVC(
            kernel="linear",
            C=1.0,
            probability=True,
            class_weight="balanced"
        ),

        "svm_rbf": SVC(
            kernel="rbf",
            C=2.0,
            gamma="scale",
            probability=True,
            class_weight="balanced"
        ),

        "rf": RandomForestClassifier(
            n_estimators=400,
            max_depth=None,
            class_weight="balanced_subsample",
            bootstrap=True,
            n_jobs=-1
        ),

        "gb": GradientBoostingClassifier(
            n_estimators=300,
            learning_rate=0.05,
            max_depth=3,
            subsample=0.9
        )
    }
    return models


############################################################
# TRAIN + EVAL FUNCTIONS
############################################################

def fit_model(model, X_train, y_train):
    """Fits a model with probability calibration."""
    base = model
    calibrated = CalibratedClassifierCV(base, cv=5, method="sigmoid")
    calibrated.fit(X_train, y_train)
    return calibrated


def evaluate_model(model, X_test, y_test) -> Dict[str, Any]:
    """Produces full metric dictionary."""
    y_prob = model.predict_proba(X_test)[:, 1]
    y_pred = (y_prob >= 0.5).astype(int)

    acc = accuracy_score(y_test, y_pred)
    auc = roc_auc_score(y_test, y_prob)
    cr = classification_report(y_test, y_pred, output_dict=True)
    cm = confusion_matrix(y_test, y_pred)

    # PR curve
    precision, recall, _ = precision_recall_curve(y_test, y_prob)

    # ROC curve
    fpr, tpr, _ = roc_curve(y_test, y_prob)

    return {
        "accuracy": float(acc),
        "auc": float(auc),
        "classification_report": cr,
        "confusion_matrix": cm.tolist(),
        "precision": precision.tolist(),
        "recall": recall.tolist(),
        "fpr": fpr.tolist(),
        "tpr": tpr.tolist(),
        "probs": y_prob.tolist(),
    }


############################################################
# CROSS VALIDATION RUNNER
############################################################

def cross_validate(model, X, y, folds=5) -> Dict[str, Any]:
    """Runs K-fold stratified CV and returns average metrics."""
    skf = StratifiedKFold(n_splits=folds, shuffle=True, random_state=42)

    aucs = []
    accs = []

    for train_idx, test_idx in skf.split(X, y):
        Xtr, Xte = X[train_idx], X[test_idx]
        ytr, yte = y[train_idx], y[test_idx]

        m = fit_model(model, Xtr, ytr)
        met = evaluate_model(m, Xte, yte)

        aucs.append(met["auc"])
        accs.append(met["accuracy"])

    return {
        "cv_auc_mean": float(np.mean(aucs)),
        "cv_auc_std": float(np.std(aucs)),
        "cv_acc_mean": float(np.mean(accs)),
        "cv_acc_std": float(np.std(accs)),
    }


############################################################
# FEATURE IMPORTANCE EXTRACTION
############################################################

def compute_feature_importance(model, feature_names) -> Dict[str, float]:
    """
    Supports:
    - Random Forest
    - Gradient Boosting
    - Linear SVM (coef)
    - Logistic Regression (coef)
    """
    if hasattr(model, "feature_importances_"):
        # Tree models
        importances = model.feature_importances_
    elif hasattr(model, "coef_"):
        # Linear models (absolute coefficient magnitude)
        importances = np.abs(model.coef_).flatten()
    else:
        return {}

    imp_dict = {}
    for name, score in zip(feature_names, importances):
        imp_dict[name] = float(score)

    # Sorted dictionary
    return dict(sorted(imp_dict.items(), key=lambda x: x[1], reverse=True))


############################################################
# MASTER TRAINING WRAPPER
############################################################

def run_classical_training(
    X_train: np.ndarray,
    y_train: np.ndarray,
    X_test: np.ndarray,
    y_test: np.ndarray,
    feature_names: list,
    out_dir: str = "classical_models"
) -> Dict[str, Any]:

    ensure_dir(out_dir)

    models = build_models()
    full_summary = {}

    for name, model in models.items():
        print(f"\n=== Training {name} ===")

        # Cross validation
        cv_metrics = cross_validate(model, X_train, y_train)
        print(f"{name} CV AUC:", cv_metrics["cv_auc_mean"])

        # Fit full model
        fitted = fit_model(model, X_train, y_train)

        # Evaluate
        metrics = evaluate_model(fitted, X_test, y_test)
        print(f"{name} Test AUC:", metrics["auc"])

        # Feature importance (if possible)
        try:
            true_model = fitted.base_estimator
            fi = compute_feature_importance(true_model, feature_names)
        except Exception:
            fi = {}

        # Save model
        dump(fitted, os.path.join(out_dir, f"{name}.joblib"))

        # Save metrics
        save_json(os.path.join(out_dir, f"{name}_metrics.json"), metrics)
        save_json(os.path.join(out_dir, f"{name}_cv.json"), cv_metrics)
        save_json(os.path.join(out_dir, f"{name}_feat_importance.json"), fi)

        full_summary[name] = {
            "cv": cv_metrics,
            "test": metrics,
            "feature_importance": fi
        }

    save_json(os.path.join(out_dir, "summary.json"), full_summary)
    print("\n=== All classical models complete. ===")
    return full_summary


############################################################
# QUICK LOADERS
############################################################

def load_model(model_path: str):
    return load(model_path)

def predict_proba(model, X):
    return model.predict_proba(X)[:, 1]

# [PART 8/12]
# ----------------------------------------------------------------------
# DEEP LEARNING TRAINING SUITE FOR SEQUENCE FEATURES (X_seq)
# - Training wrappers for CNN / BiLSTM / CNN-BiLSTM / PRCNN / Transformer
# - GPU detection, mixed precision support
# - Optimizers with gradient clipping, LR schedulers
# - EarlyStopping, ModelCheckpoint, ReduceLROnPlateau, TensorBoard
# - Evaluation, ROC/PR plots, confusion matrix
# - Save models, histories, and metrics
# ----------------------------------------------------------------------

import os
import math
import json
import time
import numpy as np
import matplotlib.pyplot as plt
from sklearn.metrics import roc_auc_score, roc_curve, precision_recall_curve, confusion_matrix, accuracy_score, precision_score, recall_score, f1_score
import tensorflow as tf
from tensorflow.keras import optimizers, callbacks

# NOTE: This part expects helper functions/models defined in previous parts:
# - ensure_dir(), write_json()
# - build_cnn_model(), build_bilstm_model(), build_cnn_bilstm_hybrid(), build_prcnn_parallel(), build_transformer_encoder()
# - SequenceDataGenerator (if using generator), compile_and_train(), evaluate()
# If any are missing because you pasted parts in a different order, paste Part 3 and Part 6 first.

# -----------------------------
# GPU & mixed-precision helpers
# -----------------------------
def available_gpus():
    gpus = tf.config.experimental.list_physical_devices('GPU')
    return [g.name for g in gpus]

def enable_mixed_precision_if_requested(enable: bool):
    if enable:
        try:
            from tensorflow.keras import mixed_precision
            mixed_precision.set_global_policy('mixed_float16')
            print("[INFO] Mixed precision enabled (float16 compute where supported).")
        except Exception as e:
            print("[WARN] Could not enable mixed precision:", e)

# -----------------------------
# Optimizer + scheduler factory
# -----------------------------
def make_optimizer_and_callbacks(out_dir: str,
                                 initial_lr: float = 1e-4,
                                 use_clipnorm: float = None,
                                 use_clipvalue: float = None,
                                 reduce_on_plateau: bool = True,
                                 plateau_patience: int = 5,
                                 monitor: str = 'val_loss'):
    """
    Returns a compiled optimizer (to be passed to model.compile) and a list of Keras callbacks.
    We prefer to set clipping on the optimizer (clipnorm or clipvalue).
    """
    if use_clipnorm:
        opt = optimizers.Adam(learning_rate=initial_lr, clipnorm=float(use_clipnorm))
    elif use_clipvalue:
        opt = optimizers.Adam(learning_rate=initial_lr, clipvalue=float(use_clipvalue))
    else:
        opt = optimizers.Adam(learning_rate=initial_lr)

    cbs = []
    ensure_dir(out_dir)
    # Model checkpoint
    ckpt = callbacks.ModelCheckpoint(os.path.join(out_dir, 'best_model.h5'),
                                     monitor=monitor, save_best_only=True, verbose=1)
    cbs.append(ckpt)
    # Early stopping
    es = callbacks.EarlyStopping(monitor=monitor, patience=10, restore_best_weights=True, verbose=1)
    cbs.append(es)
    # Reduce LR on plateau
    if reduce_on_plateau:
        rl = callbacks.ReduceLROnPlateau(monitor=monitor, factor=0.5, patience=plateau_patience, min_lr=1e-7, verbose=1)
        cbs.append(rl)
    # TensorBoard
    tb = callbacks.TensorBoard(log_dir=os.path.join(out_dir, 'logs'))
    cbs.append(tb)

    return opt, cbs

# -----------------------------
# Training + evaluation helpers
# -----------------------------
def compute_class_weights(y):
    classes = np.unique(y)
    cw = compute_class_weight('balanced', classes=classes, y=y)
    return {int(c): float(w) for c, w in zip(classes, cw)}

def evaluate_and_save(model, X_test, y_test, out_dir, prefix="test"):
    preds_proba = model.predict(X_test).ravel()
    preds = (preds_proba >= 0.5).astype(int)
    auc = float(roc_auc_score(y_test, preds_proba)) if len(np.unique(y_test))>1 else 0.0
    acc = float(accuracy_score(y_test, preds))
    prec = float(precision_score(y_test, preds, zero_division=0))
    rec = float(recall_score(y_test, preds, zero_division=0))
    f1 = float(f1_score(y_test, preds, zero_division=0))
    cm = confusion_matrix(y_test, preds).tolist()

    metrics = {
        "auc": auc, "accuracy": acc, "precision": prec, "recall": rec, "f1": f1, "confusion_matrix": cm
    }
    write_json(os.path.join(out_dir, f"{prefix}_metrics.json"), metrics)

    # Save ROC and PR curves
    try:
        fpr, tpr, _ = roc_curve(y_test, preds_proba)
        plt.figure()
        plt.plot(fpr, tpr, label=f"AUC={auc:.3f}")
        plt.plot([0,1],[0,1],'--', color='gray')
        plt.xlabel("FPR"); plt.ylabel("TPR"); plt.title("ROC")
        plt.legend(); plt.tight_layout()
        plt.savefig(os.path.join(out_dir, f"{prefix}_roc.png")); plt.close()
    except Exception:
        pass

    try:
        precision, recall, _ = precision_recall_curve(y_test, preds_proba)
        plt.figure()
        plt.plot(recall, precision)
        plt.xlabel("Recall"); plt.ylabel("Precision"); plt.title("PR Curve")
        plt.tight_layout()
        plt.savefig(os.path.join(out_dir, f"{prefix}_pr.png")); plt.close()
    except Exception:
        pass

    # Confusion matrix plot
    try:
        cm_arr = np.array(cm)
        plt.figure(figsize=(4,4))
        sns.heatmap(cm_arr, annot=True, fmt='d', xticklabels=['HC','AD'], yticklabels=['HC','AD'], cmap='Blues')
        plt.xlabel('Predicted'); plt.ylabel('True'); plt.title('Confusion Matrix')
        plt.tight_layout(); plt.savefig(os.path.join(out_dir, f"{prefix}_confusion.png")); plt.close()
    except Exception:
        pass

    return metrics

# -----------------------------
# High-level trainer for a sequence model
# -----------------------------
def train_sequence_model(model_builder_fn,
                         X_seq: np.ndarray,
                         y: np.ndarray,
                         out_dir: str,
                         model_name: str,
                         val_split: float = 0.15,
                         epochs: int = 40,
                         batch_size: int = 16,
                         initial_lr: float = 1e-4,
                         use_clipnorm: float = 1.0,
                         mixed_precision: bool = False,
                         use_generator: bool = False,
                         augment: bool = False,
                         seed: int = RANDOM_SEED):
    """
    Train a model using model_builder_fn(input_shape)->tf.keras.Model
    X_seq: array (N, T, D)
    y: array (N,)
    Saves artifacts to out_dir/model_name/
    """
    np.random.seed(seed)
    tf.random.set_seed(seed)
    ensure_dir(out_dir)
    model_dir = os.path.join(out_dir, model_name)
    ensure_dir(model_dir)

    # Split train/val/test: we'll keep test split externally; here we do train/val only.
    X_train, X_val, y_train, y_val = train_test_split(X_seq, y, test_size=val_split, stratify=y, random_state=seed)

    input_shape = (X_seq.shape[1], X_seq.shape[2])
    model = model_builder_fn(input_shape)

    # Mixed precision if requested
    enable_mixed_precision_if_requested(mixed_precision)

    # Optimizer & callbacks
    opt, cbs = make_optimizer_and_callbacks(model_dir, initial_lr=initial_lr, use_clipnorm=use_clipnorm, monitor='val_loss')
    model.compile(optimizer=opt, loss='binary_crossentropy', metrics=['accuracy'])

    # class weights
    cw = compute_class_weights(y_train)

    # Training: optionally use generator
    if use_generator:
        train_gen = SequenceDataGenerator(X_train, y_train, batch_size=batch_size, shuffle=True, augment=augment)
        val_gen = SequenceDataGenerator(X_val, y_val, batch_size=batch_size, shuffle=False, augment=False)
        history = model.fit(train_gen, validation_data=val_gen, epochs=epochs, callbacks=cbs, class_weight=cw, verbose=2)
    else:
        history = model.fit(X_train, y_train, validation_data=(X_val, y_val), epochs=epochs, batch_size=batch_size, callbacks=cbs, class_weight=cw, verbose=2)

    # Save history
    hist_path = os.path.join(model_dir, 'history.json')
    write_json(hist_path, {k: [float(v) for v in vals] for k, vals in history.history.items()})

    # Load best model and return it
    best_fp = os.path.join(model_dir, 'best_model.h5')
    final_model_fp = os.path.join(model_dir, 'final_model.h5')
    try:
        if os.path.exists(best_fp):
            trained = tf.keras.models.load_model(best_fp)
        else:
            trained = model
        trained.save(final_model_fp)
    except Exception as e:
        print("[WARN] Could not load/save final model:", e)
        trained = model

    # Evaluate on validation set and save metrics
    val_metrics = evaluate_and_save(trained, X_val, y_val, model_dir, prefix="val")
    print(f"[INFO] Validation metrics for {model_name}:", val_metrics)
    return trained, val_metrics, history.history

# -----------------------------
# Multi-model trainer (train many architectures and compare)
# -----------------------------
def run_all_sequence_models(X_seq: np.ndarray, y: np.ndarray, out_dir: str,
                            epochs: int = 40, batch_size: int = 16,
                            mixed_precision: bool = False):
    """
    Trains CNN, BiLSTM, CNN-BiLSTM, PRCNN, Transformer on the same dataset.
    Saves per-model results under out_dir/<model_name>/
    Returns summary dictionary.
    """
    ensure_dir(out_dir)
    summary = {}

    builders = {
        "CNN": build_cnn_model,
        "BiLSTM": build_bilstm_model,
        "CNN_BiLSTM": build_cnn_bilstm_hybrid,
        "PRCNN": build_prcnn_parallel,
        "Transformer": build_transformer_encoder
    }

    for name, builder in builders.items():
        print(f"\n================ Training {name} ================\n")
        try:
            model_obj, metrics, hist = train_sequence_model(
                model_builder_fn=builder,
                X_seq=X_seq,
                y=y,
                out_dir=out_dir,
                model_name=name,
                epochs=epochs,
                batch_size=batch_size,
                initial_lr=1e-4,
                use_clipnorm=1.0,
                mixed_precision=mixed_precision,
                use_generator=False,
                augment=False
            )
            summary[name] = {"metrics": metrics, "history": hist}
        except Exception as e:
            print(f"[ERROR] Training {name} failed: {e}")
            summary[name] = {"error": str(e)}

    write_json(os.path.join(out_dir, "sequence_models_summary.json"), summary)
    return summary

# -----------------------------
# Utility to load saved model and run test evaluation
# -----------------------------
def evaluate_saved_model(model_fp: str, X_test: np.ndarray, y_test: np.ndarray, out_dir: str, prefix: str = "test"):
    ensure_dir(out_dir)
    model = tf.keras.models.load_model(model_fp)
    metrics = evaluate_and_save(model, X_test, y_test, out_dir, prefix=prefix)
    return metrics

# ----------------------------------------------------------------------
# End of PART 8/12
# ----------------------------------------------------------------------
# [PART 9/12]
# ----------------------------------------------------------------------
# EVALUATION & EXPLAINABILITY MODULE
# - Comprehensive evaluation utilities (metrics, ROC/PR/CM plotting)
# - SHAP explainability for classical models (if available)
# - Saliency maps for CNN models
# - Attention visualization hooks for Transformer
# - Single-audio inference pipeline (preprocess -> features -> model -> predict)
# - CLI wrapper to run evaluation & explanations on a saved dataset
# ----------------------------------------------------------------------

import os
import json
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from typing import Any, Dict, Tuple, List

from sklearn.metrics import (
    roc_auc_score, roc_curve, precision_recall_curve,
    confusion_matrix, accuracy_score, precision_score, recall_score, f1_score
)

# Optional SHAP
try:
    import shap
    HAS_SHAP = True
except Exception:
    HAS_SHAP = False

# TensorFlow for saliency
import tensorflow as tf

# Reuse helpers from previous parts:
# ensure_dir, write_json, load_feature_matrices, load_feature_npz, extract_all_features, pad_truncate_sequences, sanitize_feature_dict

def compute_basic_metrics(y_true: np.ndarray, y_probs: np.ndarray, threshold: float = 0.5) -> Dict[str, Any]:
    y_pred = (y_probs >= threshold).astype(int)
    acc = accuracy_score(y_true, y_pred)
    prec = precision_score(y_true, y_pred, zero_division=0)
    rec = recall_score(y_true, y_pred, zero_division=0)
    f1 = f1_score(y_true, y_pred, zero_division=0)
    auc = float(roc_auc_score(y_true, y_probs)) if len(np.unique(y_true))>1 else 0.0
    cm = confusion_matrix(y_true, y_pred).tolist()
    return {"accuracy": float(acc), "precision": float(prec), "recall": float(rec), "f1": float(f1), "auc": auc, "cm": cm}

def plot_and_save_roc_pr(y_true: np.ndarray, y_scores: np.ndarray, out_dir: str, prefix: str = "model"):
    ensure_dir(out_dir)
    # ROC
    try:
        fpr, tpr, _ = roc_curve(y_true, y_scores)
        auc = roc_auc_score(y_true, y_scores)
        plt.figure()
        plt.plot(fpr, tpr, label=f"AUC={auc:.3f}")
        plt.plot([0,1],[0,1],'--', color='gray')
        plt.xlabel("False Positive Rate"); plt.ylabel("True Positive Rate"); plt.title(f"ROC - {prefix}")
        plt.legend(); plt.tight_layout()
        plt.savefig(os.path.join(out_dir, f"{prefix}_roc.png")); plt.close()
    except Exception:
        pass

    # PR
    try:
        precision, recall, _ = precision_recall_curve(y_true, y_scores)
        ap = np.trapz(precision, recall)
        plt.figure()
        plt.plot(recall, precision, label=f"AP={ap:.3f}")
        plt.xlabel("Recall"); plt.ylabel("Precision"); plt.title(f"PR - {prefix}")
        plt.legend(); plt.tight_layout()
        plt.savefig(os.path.join(out_dir, f"{prefix}_pr.png")); plt.close()
    except Exception:
        pass

def plot_and_save_confusion(cm: List[List[int]], out_dir: str, prefix: str = "model", labels: List[str] = ["HC","AD"]):
    ensure_dir(out_dir)
    try:
        cm_arr = np.array(cm)
        plt.figure(figsize=(4,4))
        sns.heatmap(cm_arr, annot=True, fmt='d', xticklabels=labels, yticklabels=labels, cmap='Blues')
        plt.xlabel("Predicted"); plt.ylabel("True"); plt.title(f"Confusion Matrix - {prefix}")
        plt.tight_layout()
        plt.savefig(os.path.join(out_dir, f"{prefix}_confusion.png")); plt.close()
    except Exception:
        pass

# ------------------------
# SHAP explainability for classical models (RF, GB)
# ------------------------
def explain_with_shap(model, X_background: np.ndarray, X_explain: np.ndarray, feature_names: List[str], out_dir: str, name: str = "shap"):
    """
    Produces SHAP summary plot and per-sample waterfall for top features.
    - For tree models, use TreeExplainer; otherwise KernelExplainer (slower).
    """
    ensure_dir(out_dir)
    if not HAS_SHAP:
        print("[WARN] SHAP not installed. Skipping SHAP explanations.")
        return

    try:
        if hasattr(shap, "TreeExplainer") and (hasattr(model, "feature_importances_") or "xgb" in model.__class__.__name__.lower()):
            explainer = shap.TreeExplainer(model)
        else:
            # KernelExplainer needs a prediction function and background sample
            explainer = shap.KernelExplainer(lambda x: model.predict_proba(x)[:,1], X_background[:100])
        shap_values = explainer.shap_values(X_explain, nsamples=100 if not hasattr(explainer, "shap_values") else "auto")
        # summary
        try:
            plt.figure()
            shap.summary_plot(shap_values, X_explain, feature_names=feature_names, show=False)
            plt.savefig(os.path.join(out_dir, f"{name}_summary.png")); plt.close()
        except Exception:
            pass
        # save numeric values
        np.savez(os.path.join(out_dir, f"{name}_shap.npz"), shap_values=shap_values, X=X_explain)
    except Exception as e:
        print("[WARN] SHAP explanation failed:", e)

# ------------------------
# Saliency map for 1D-CNN (per-frame importance)
# ------------------------
def saliency_for_cnn(model: tf.keras.Model, X_sample: np.ndarray, out_dir: str, prefix: str = "saliency"):
    """
    Computes gradient-based saliency (Integrated Gradients style simple approx).
    X_sample: (T, D) or (1, T, D)
    Produces a heatmap of per-frame importance (time x features aggregated).
    """
    ensure_dir(out_dir)
    try:
        if X_sample.ndim == 2:
            inp = X_sample[np.newaxis, ...]
        else:
            inp = X_sample
        inp_tf = tf.convert_to_tensor(inp, dtype=tf.float32)
        with tf.GradientTape() as tape:
            tape.watch(inp_tf)
            preds = model(inp_tf, training=False)
            pred = preds[:, 0]
        grads = tape.gradient(pred, inp_tf)  # shape (1,T,D)
        grads = tf.abs(grads).numpy().squeeze()
        # aggregate over features to get per-frame importance
        frame_imp = np.mean(grads, axis=-1)
        # plot heatmap
        plt.figure(figsize=(12,3))
        plt.imshow(frame_imp[np.newaxis, :], aspect="auto", interpolation='nearest', cmap='hot')
        plt.colorbar(); plt.title(f"Saliency per frame - {prefix}")
        plt.xlabel("Frame"); plt.yticks([])
        plt.tight_layout()
        plt.savefig(os.path.join(out_dir, f"{prefix}_frame_saliency.png")); plt.close()
        # also save arrays
        np.savez(os.path.join(out_dir, f"{prefix}_saliency.npz"), frame_importance=frame_imp, grads=grads)
    except Exception as e:
        print("[WARN] Saliency computation failed:", e)

# ------------------------
# Attention visualization for Transformer models
# ------------------------
def visualize_transformer_attention(model: tf.keras.Model, X_sample: np.ndarray, out_dir: str, prefix: str = "attention"):
    """
    Attempts to extract attention weights from a Transformer encoder model that exposes them.
    This requires that the MultiHeadAttention layers were created with 'return_attention_scores=True'
    and that the model is built to output attention maps or store them as attributes.
    If not available, this function will gracefully skip.
    """
    ensure_dir(out_dir)
    try:
        # Try to call the model with training=False and inspect attention in layers
        # Many Keras MultiHeadAttention layers do not return attention by default,
        # so this is best-effort and may require model modification at build time.
        attention_maps = []
        inp = X_sample[np.newaxis, ...] if X_sample.ndim == 2 else X_sample
        # Walk model layers and attempt to call them with return_attention_scores where supported
        for layer in model.layers:
            if isinstance(layer, tf.keras.layers.MultiHeadAttention):
                # reconstruct query/key/value to get attention scores
                try:
                    q = tf.convert_to_tensor(inp, dtype=tf.float32)
                    k = q; v = q
                    attn_out, attn_scores = layer(q, k, v, return_attention_scores=True)
                    attention_maps.append(attn_scores.numpy())
                except Exception:
                    continue
        if not attention_maps:
            print("[INFO] No attention maps found in model layers; model must be built to return them.")
            return
        # For simplicity visualize the first head of the first returned map
        att = attention_maps[0]  # shape (batch, heads, seq, seq)
        att_head0 = att[0, 0]  # (seq, seq)
        plt.figure(figsize=(6,6))
        sns.heatmap(att_head0, cmap='viridis')
        plt.title(f"Attention head 0 - {prefix}")
        plt.xlabel("Key frame"); plt.ylabel("Query frame")
        plt.tight_layout()
        plt.savefig(os.path.join(out_dir, f"{prefix}_head0.png")); plt.close()
        np.save(os.path.join(out_dir, f"{prefix}_attn.npy"), att)
    except Exception as e:
        print("[WARN] Attention visualization failed:", e)

# ------------------------
# Single-audio inference helper (end-to-end)
# ------------------------
def inference_single_audio(audio_path: str,
                           model_type: str,
                           model_fp: str,
                           scaler: Any = None,
                           seq_builder_fn = None,
                           max_seq_len: int = 800,
                           return_raw: bool = False) -> Dict[str, Any]:
    """
    End-to-end inference for a single WAV file.
    - audio_path: path to WAV
    - model_type: 'classical' or 'sequence'
    - model_fp: path to saved model (joblib for classical, .h5 for keras)
    - scaler: optional FeatureScaler object to normalize aggregate features
    - seq_builder_fn: function that given raw audio returns sequence features (T,D)
    Returns dictionary with probability, label(0/1) and intermediate features.
    """
    # 1. Load audio & extract features (uses existing extract_all_features or build_dataset utilities)
    try:
        y, sr = safe_load_wav(audio_path)
    except Exception:
        # try librosa load fallback
        import librosa
        y, sr = librosa.load(audio_path, sr=DEFAULT_SR)

    feats = extract_all_features(y, sr, limit_frames=max_seq_len)  # from PART 2
    agg_feats = feats.get("aggregate", {})
    agg_feats = sanitize_feature_dict(agg_feats)

    if scaler:
        agg_scaled = scaler.transform(agg_feats, method="zscore")
        agg_vector, feat_names = build_aggregate_vector(agg_scaled, feats.get("prosody", {}), feats.get("pauses", {}), feats.get("dysfluency", {}))
    else:
        agg_vector, feat_names = build_aggregate_vector(agg_feats, feats.get("prosody", {}), feats.get("pauses", {}), feats.get("dysfluency", {}))

    # 2. Load model & predict
    result = {"audio_path": audio_path, "features": {}}
    if model_type == "classical":
        # joblib load
        from joblib import load
        clf = load(model_fp)
        prob = clf.predict_proba(agg_vector.reshape(1, -1))[:,1].item()
        result.update({"probability": float(prob), "label": int(prob >= 0.5), "agg_vector": agg_vector.tolist()})
        return result
    elif model_type == "sequence":
        # sequence model expects padded (1,T,D)
        seq = fuse_sequence_features(feats["mfcc"], feats["gtcc"], feats["spectral"])
        seq_padded = pad_truncate_sequences([seq], max_seq_len)[0][np.newaxis, ...]  # shape (1,T,D)
        model = tf.keras.models.load_model(model_fp)
        prob = float(model.predict(seq_padded).ravel()[0])
        result.update({"probability": prob, "label": int(prob >= 0.5), "sequence_shape": seq.shape})
        if return_raw:
            result["sequence"] = seq.tolist()
        return result
    else:
        raise ValueError("Unknown model_type. Choose 'classical' or 'sequence'.")

# ------------------------
# CLI: evaluate saved models on dataset (X_seq / X_agg)
# ------------------------
def cli_evaluate(args):
    """
    args should provide:
        - feature_dir (output of PART 6)
        - models_dir (where models are saved)
        - out_dir (where evaluation images & JSON are saved)
    """
    feature_dir = args.feature_dir
    models_dir = args.models_dir
    out_dir = args.out_dir
    ensure_dir(out_dir)

    X_seq, X_agg, y = load_feature_matrices(feature_dir)
    # Load and evaluate classical models (joblib)
    classical_models = glob.glob(os.path.join(models_dir, "*.joblib"))
    for mfp in classical_models:
        name = os.path.basename(mfp).replace(".joblib", "")
        try:
            from joblib import load
            clf = load(mfp)
            probs = clf.predict_proba(X_agg)[:,1]
            metrics = compute_basic_metrics(y, probs)
            write_json(os.path.join(out_dir, f"{name}_metrics.json"), metrics)
            plot_and_save_roc_pr(y, probs, out_dir, prefix=name)
            plot_and_save_confusion(metrics["cm"], out_dir, prefix=name)
            print(f"[INFO] Evaluated classical model {name}.")
        except Exception as e:
            print(f"[WARN] Could not evaluate {mfp}: {e}")

    # Keras sequence models (.h5)
    keras_models = glob.glob(os.path.join(models_dir, "*.h5"))
    for mfp in keras_models:
        name = os.path.basename(mfp).replace(".h5", "")
        try:
            model = tf.keras.models.load_model(mfp)
            probs = model.predict(X_seq).ravel()
            metrics = compute_basic_metrics(y, probs)
            write_json(os.path.join(out_dir, f"{name}_metrics.json"), metrics)
            plot_and_save_roc_pr(y, probs, out_dir, prefix=name)
            plot_and_save_confusion(metrics["cm"], out_dir, prefix=name)
            print(f"[INFO] Evaluated keras model {name}.")
        except Exception as e:
            print(f"[WARN] Could not evaluate {mfp}: {e}")

# ----------------------------------------------------------------------
# End of PART 9/12
# ----------------------------------------------------------------------
# ================================================================
#  SECTION 10 — TRAINING UTILITIES FOR DEEP LEARNING MODELS
# ================================================================

import numpy as np
import torch
import torch.nn as nn
from torch.utils.data import Dataset, DataLoader
from sklearn.model_selection import train_test_split
from torch.optim import Adam
from torch.optim.lr_scheduler import ReduceLROnPlateau
import time
import json
import matplotlib.pyplot as plt


# ----------------------------------------------------------------
# 10.1 Dataset Wrapper
# ----------------------------------------------------------------
class SpeechFeatureDataset(Dataset):
    def __init__(self, X, y):
        self.X = torch.tensor(X, dtype=torch.float32)
        self.y = torch.tensor(y, dtype=torch.float32)

    def __len__(self):
        return len(self.X)

    def __getitem__(self, idx):
        return self.X[idx], self.y[idx]


# ----------------------------------------------------------------
# 10.2 Save Training Metrics
# ----------------------------------------------------------------
def save_training_metrics(history, save_path):
    with open(save_path, "w") as f:
        json.dump(history, f, indent=4)


# ----------------------------------------------------------------
# 10.3 Plot Training Curves
# ----------------------------------------------------------------
def plot_training_curves(history, save_path):
    plt.figure(figsize=(10,6))
    plt.plot(history["train_loss"], label="Train Loss")
    plt.plot(history["val_loss"], label="Validation Loss")
    plt.xlabel("Epoch")
    plt.ylabel("Loss")
    plt.legend()
    plt.grid()
    plt.tight_layout()
    plt.savefig(save_path)
    plt.close()
    

# ----------------------------------------------------------------
# 10.4 Train Model (Generalized for ANY NN)
# ----------------------------------------------------------------
def train_model(
    model,
    X,
    y,
    save_dir,
    batch_size=32,
    test_size=0.2,
    lr=1e-3,
    epochs=30,
    patience=5,
    device="cuda" if torch.cuda.is_available() else "cpu"
):

    print(f"\n[INFO] Starting training on: {device}")

    # -----------------------------
    # Prepare data
    # -----------------------------
    X_train, X_val, y_train, y_val = train_test_split(
        X, y, test_size=test_size, random_state=42, shuffle=True
    )

    train_ds = SpeechFeatureDataset(X_train, y_train)
    val_ds   = SpeechFeatureDataset(X_val, y_val)

    train_loader = DataLoader(train_ds, batch_size=batch_size, shuffle=True)
    val_loader   = DataLoader(val_ds, batch_size=batch_size, shuffle=False)

    # -----------------------------
    # Move model to device
    # -----------------------------
    model = model.to(device)

    # Loss, optimizer, scheduler
    criterion = nn.BCELoss()
    optimizer = Adam(model.parameters(), lr=lr)
    scheduler = ReduceLROnPlateau(optimizer, factor=0.5, patience=2)

    history = {"train_loss": [], "val_loss": []}
    best_loss = float("inf")
    best_model_path = os.path.join(save_dir, "best_model.pt")
    os.makedirs(save_dir, exist_ok=True)

    # -----------------------------
    # Training Loop
    # -----------------------------
    for epoch in range(1, epochs + 1):
        model.train()
        running_loss = 0

        for X_batch, y_batch in train_loader:
            X_batch = X_batch.to(device)
            y_batch = y_batch.to(device)

            optimizer.zero_grad()
            preds = model(X_batch)
            loss = criterion(preds.squeeze(), y_batch)
            loss.backward()
            optimizer.step()

            running_loss += loss.item()

        train_loss = running_loss / len(train_loader)

        # -----------------------------
        # Validation Phase
        # -----------------------------
        model.eval()
        val_loss = 0

        with torch.no_grad():
            for X_batch, y_batch in val_loader:
                X_batch = X_batch.to(device)
                y_batch = y_batch.to(device)

                preds = model(X_batch)
                loss = criterion(preds.squeeze(), y_batch)
                val_loss += loss.item()

        val_loss /= len(val_loader)

        # Store history
        history["train_loss"].append(train_loss)
        history["val_loss"].append(val_loss)

        print(
            f"Epoch {epoch}/{epochs} | "
            f"Train Loss: {train_loss:.4f} | "
            f"Val Loss: {val_loss:.4f}"
        )

        # Scheduler update
        scheduler.step(val_loss)

        # -----------------------------
        # Early Stopping & Save Best Model
        # -----------------------------
        if val_loss < best_loss:
            best_loss = val_loss
            torch.save(model.state_dict(), best_model_path)
            wait = 0
            print(f" → Saved new best model (val_loss={val_loss:.4f})")
        else:
            wait += 1
            if wait >= patience:
                print(" → Early stopping triggered.")
                break

    # Save training curves + metrics
    save_training_metrics(history, os.path.join(save_dir, "training_history.json"))
    plot_training_curves(history, os.path.join(save_dir, "training_curve.png"))

    print("\n[INFO] Training finished.")
    print(f"[INFO] Best model saved at: {best_model_path}")

    return best_model_path
# [PART 11/12]
# ----------------------------------------------------------------------
# PART 11 — ADVANCED EVALUATION, DEPLOYMENT HELPERS & CLI ORCHESTRATION
# - Final model evaluation helpers (per-split and cross-dataset)
# - Model ensemble utilities (stacking, averaging)
# - SHAP/attention/saliency orchestration (calls PART 9 explainability functions)
# - CLI entrypoint functions to run full pipeline: build -> train -> evaluate -> explain
# - Lightweight Flask inference server stub for local testing
# ----------------------------------------------------------------------

import os
import json
import time
import glob
import numpy as np
from typing import Dict, Any, List, Tuple

# Reuse utilities from earlier parts:
# ensure_dir, write_json, load_feature_matrices, build_feature_matrices,
# run_all_sequence_models, run_classical_training, evaluate_saved_model,
# inference_single_audio, explain_with_shap, saliency_for_cnn, visualize_transformer_attention

# ------------------------
# Ensemble utilities
# ------------------------
def average_ensemble(predictions_list: List[np.ndarray]) -> np.ndarray:
    """
    Given a list of prediction vectors (each shape (N,)), returns the average ensemble probability.
    """
    if not predictions_list:
        return np.array([])
    stacked = np.stack(predictions_list, axis=0)  # (M, N)
    return np.mean(stacked, axis=0)

def stacking_ensemble(oof_preds: np.ndarray, oof_labels: np.ndarray, test_preds_list: List[np.ndarray]) -> np.ndarray:
    """
    Simple stacking using logistic regression as meta-learner.
    - oof_preds: (N_train, M) predictions from base models on training (out-of-fold)
    - oof_labels: (N_train,)
    - test_preds_list: list of arrays (N_test,) for base models
    Returns meta-level predictions for test set (N_test,)
    """
    from sklearn.linear_model import LogisticRegression
    M = oof_preds.shape[1]
    meta = LogisticRegression(max_iter=1000, class_weight='balanced')
    meta.fit(oof_preds, oof_labels)
    test_mat = np.stack(test_preds_list, axis=1)  # (N_test, M)
    return meta.predict_proba(test_mat)[:,1]

# ------------------------
# Full evaluation across splits (train/val/test)
# ------------------------
def evaluate_all_models(models_dir: str, feature_dir: str, out_dir: str):
    """
    Loads X_seq, X_agg, y from feature_dir, and evaluates all models found in models_dir.
    Produces JSON summary and plots saved under out_dir.
    """
    ensure_dir(out_dir)
    X_seq, X_agg, y = load_feature_matrices(feature_dir)

    results = {}

    # Evaluate classical models
    for joblib_fp in glob.glob(os.path.join(models_dir, "*.joblib")):
        name = os.path.splitext(os.path.basename(joblib_fp))[0]
        try:
            from joblib import load
            clf = load(joblib_fp)
            probs = clf.predict_proba(X_agg)[:,1]
            metrics = compute_basic_metrics(y, probs)
            write_json(os.path.join(out_dir, f"{name}_metrics.json"), metrics)
            plot_and_save_roc_pr(y, probs, out_dir, prefix=name)
            results[name] = metrics
        except Exception as e:
            results[name] = {"error": str(e)}

    # Evaluate Keras models
    for h5_fp in glob.glob(os.path.join(models_dir, "*.h5")):
        name = os.path.splitext(os.path.basename(h5_fp))[0]
        try:
            model = tf.keras.models.load_model(h5_fp)
            probs = model.predict(X_seq).ravel()
            metrics = compute_basic_metrics(y, probs)
            write_json(os.path.join(out_dir, f"{name}_metrics.json"), metrics)
            plot_and_save_roc_pr(y, probs, out_dir, prefix=name)
            results[name] = metrics
        except Exception as e:
            results[name] = {"error": str(e)}

    write_json(os.path.join(out_dir, "evaluation_summary.json"), results)
    print("[INFO] All model evaluations saved to", out_dir)
    return results

# ------------------------
# Cross-dataset evaluation wrapper
# ------------------------
def cross_dataset_evaluate(models_dir: str, feature_dirs: List[str], out_root: str):
    """
    Evaluate each model in models_dir on multiple feature directories (different datasets).
    Creates a structured folder out_root/<dataset>/<model>_metrics.json
    """
    ensure_dir(out_root)
    for feat_dir in feature_dirs:
        dataset_name = os.path.basename(os.path.normpath(feat_dir))
        out_dir = os.path.join(out_root, dataset_name)
        ensure_dir(out_dir)
        evaluate_all_models(models_dir, feat_dir, out_dir)

# ------------------------
# Simple inference server (Flask) — for local testing only
# ------------------------
def start_flask_server(model_type: str, model_fp: str, scaler_path: str = None, seq_builder_fn = None, port: int = 5000):
    """
    Starts a minimal Flask server exposing /predict that accepts multipart/form-data 'file' (wav)
    and returns JSON {probability, label}.
    NOTE: Flask is lightweight and not production-grade. Use Gunicorn/uvicorn for production.
    """
    try:
        from flask import Flask, request, jsonify
    except Exception:
        print("[WARN] Flask not installed. Install via `pip install flask` to use the inference server.")
        return

    app = Flask(__name__)

    # Load scaler if provided
    scaler = None
    if scaler_path and os.path.exists(scaler_path):
        try:
            scaler = FeatureScaler.load(scaler_path)
        except Exception as e:
            print("[WARN] Could not load scaler:", e)

    @app.route("/predict", methods=["POST"])
    def predict_route():
        if 'file' not in request.files:
            return jsonify({"error": "No file part"}), 400
        f = request.files['file']
        tmp_path = os.path.join("/tmp", f.filename)
        f.save(tmp_path)
        try:
            res = inference_single_audio(tmp_path, model_type=model_type, model_fp=model_fp, scaler=scaler, seq_builder_fn=seq_builder_fn, return_raw=False)
            return jsonify(res)
        except Exception as e:
            return jsonify({"error": str(e)}), 500
        finally:
            try:
                os.remove(tmp_path)
            except Exception:
                pass

    print(f"[INFO] Starting Flask server on port {port} — model_type={model_type}, model_fp={model_fp}")
    app.run(host="0.0.0.0", port=port, debug=False)

# ------------------------
# Orchestration CLI (run full pipeline)
# ------------------------
def run_pipeline_cli(args):
    """
    Orchestrates the following steps in order (with safe guards):
    1) build_dataset (PART 2) -> features saved to out_dir/features
    2) build_feature_matrices (PART 6)
    3) fit FeatureScaler (PART 5) on training split and save
    4) Train classical models (PART 7) on X_agg
    5) Train sequence models (PART 8) on X_seq
    6) Evaluate all models (PART 11)
    7) Optionally start Flask server for a chosen model
    """

    data_dir = args.data_dir
    work_dir = args.work_dir
    ensure_dir(work_dir)

    features_dir = os.path.join(work_dir, "features")
    matrices_dir = os.path.join(work_dir, "matrices")
    models_dir = os.path.join(work_dir, "models")
    eval_dir = os.path.join(work_dir, "evaluation")
    ensure_dir(features_dir); ensure_dir(matrices_dir); ensure_dir(models_dir); ensure_dir(eval_dir)

    # 1) build_dataset
    print("[STEP 1] Building dataset and extracting features...")
    build_dataset(data_dir, work_dir, limit_frames=args.max_frames, augment=args.augment, use_vad=args.use_vad, diarize=args.diarize, diarizer_token=args.diarizer_token, instructor_label=args.instructor_label)

    # 2) build_feature_matrices
    print("[STEP 2] Building feature matrices...")
    summary = build_feature_matrices(features_dir, matrices_dir, max_seq_len=args.max_seq_len)
    print("Matrix build summary:", summary)

    # 3) FeatureScaler fit on training agg features
    print("[STEP 3] Fitting feature scaler on training data...")
    X_seq, X_agg, y = load_feature_matrices(matrices_dir)
    # use train_idx if exists
    try:
        train_idx = json.load(open(os.path.join(matrices_dir, "train_idx.json")))["train_idx"]
    except Exception:
        from sklearn.model_selection import train_test_split
        train_idx, _ = train_test_split(np.arange(len(y)), test_size=args.test_size, stratify=y, random_state=42)
    agg_train = X_agg[train_idx]
    # convert agg back to list-of-dicts? Instead assume we fit scaler on numeric arrays columns
    scaler = FeatureScaler()
    # build fake dicts for scaler.fit using column names from feature_names.json
    featnames = json.load(open(os.path.join(matrices_dir, "feature_names.json")))["feature_names"]
    dicts = []
    for row in agg_train:
        d = {name: float(val) for name, val in zip(featnames, row.tolist())}
        dicts.append(d)
    scaler.fit(dicts)
    scaler.save(os.path.join(models_dir, "feature_scaler.pkl"))

    # 4) Train classical models
    print("[STEP 4] Training classical ML models...")
    # split
    from sklearn.model_selection import train_test_split
    idx_train, idx_test = train_test_split(np.arange(len(y)), test_size=args.test_size, stratify=y, random_state=42)
    X_train_agg = X_agg[idx_train]; X_test_agg = X_agg[idx_test]
    y_train = y[idx_train]; y_test = y[idx_test]
    # Load feature names
    feature_names = featnames
    run_classical_training(X_train_agg, y_train, X_test_agg, y_test, feature_names, out_dir=os.path.join(models_dir, "classical"))

    # 5) Train sequence models
    print("[STEP 5] Training sequence DL models...")
    X_train_seq = X_seq[idx_train]; X_test_seq = X_seq[idx_test]
    run_all_sequence_models(X_train_seq, y_train, out_dir=os.path.join(models_dir, "sequence"), epochs=args.epochs, batch_size=args.batch_size, mixed_precision=args.mixed_precision)

    # 6) Evaluate all models
    print("[STEP 6] Evaluating models...")
    evaluate_all_models(os.path.join(models_dir, "classical"), matrices_dir, os.path.join(eval_dir, "classical"))
    evaluate_all_models(os.path.join(models_dir, "sequence"), matrices_dir, os.path.join(eval_dir, "sequence"))

    # 7) Optionally start server
    if args.serve and args.serve_model_fp:
        start_flask_server(args.serve_model_type, args.serve_model_fp, scaler_path=os.path.join(models_dir, "feature_scaler.pkl"))

    print("[PIPELINE] Completed successfully.")

# ------------------------
# CLI arg parser helper
# ------------------------
def add_run_pipeline_args(parser):
    parser.add_argument('--data_dir', required=True)
    parser.add_argument('--work_dir', required=True)
    parser.add_argument('--max_frames', type=int, default=DEFAULT_MAX_FRAMES)
    parser.add_argument('--max_seq_len', type=int, default=800)
    parser.add_argument('--augment', action='store_true')
    parser.add_argument('--use_vad', action='store_true')
    parser.add_argument('--diarize', action='store_true')
    parser.add_argument('--diarizer_token', type=str, default=None)
    parser.add_argument('--instructor_label', type=str, default=None)
    parser.add_argument('--test_size', type=float, default=0.2)
    parser.add_argument('--epochs', type=int, default=40)
    parser.add_argument('--batch_size', type=int, default=16)
    parser.add_argument('--mixed_precision', action='store_true')
    parser.add_argument('--serve', action='store_true')
    parser.add_argument('--serve_model_fp', type=str, default=None)
    parser.add_argument('--serve_model_type', type=str, default='classical')

# ----------------------------------------------------------------------
# End of PART 11/12
# ----------------------------------------------------------------------
# [PART 12/12]
# ----------------------------------------------------------------------
# FINAL PART — CLI ENTRYPOINT, README, REQUIREMENTS, & USAGE NOTES
# - Provides a consolidated `__main__` that exposes the pipeline CLI
# - Includes helpful usage examples, a minimal requirements listing,
#   and troubleshooting notes for common environment issues.
# - Place this PART 12 at the end of your `full_ad_speech_pipeline.py`
# ----------------------------------------------------------------------

"""
PART 12 — Entrypoint, README & Notes

Save the full file as: full_ad_speech_pipeline.py
Usage examples (after pasting all parts and installing requirements):

# Basic one-shot run (feature extraction, feature matrices, train, eval)
python3 full_ad_speech_pipeline.py run_pipeline --data_dir ./data --work_dir ./work --epochs 30 --batch_size 16 --augment --use_vad

# Only build features (fast check)
python3 full_ad_speech_pipeline.py build_features --data_dir ./data --work_dir ./work --max_frames 800

# Train sequence models only (assumes matrices already built)
python3 full_ad_speech_pipeline.py train_sequence --matrices_dir ./work/matrices --models_dir ./work/models --epochs 30 --batch_size 16

# Evaluate saved models on matrices
python3 full_ad_speech_pipeline.py evaluate --models_dir ./work/models --matrices_dir ./work/matrices --out_dir ./work/eval

# Start inference server (local)
python3 full_ad_speech_pipeline.py serve --model_type classical --model_fp ./work/models/classical/rf.joblib --port 5000

Notes:
- For heavy-duty components (pyannote, wav2vec2, transformers, parselmouth), install optional packages separately.
- If using GPU, ensure CUDA & cuDNN versions match your TensorFlow / PyTorch install.
"""

import argparse
import sys

# Reuse/expect functions defined in previous parts:
# - add_run_pipeline_args, run_pipeline_cli, build_dataset, build_feature_matrices,
#   run_all_sequence_models, run_classical_training, evaluate_all_models,
#   start_flask_server, inference_single_audio, ensure_dir

def create_arg_parser():
    parser = argparse.ArgumentParser(prog="full_ad_speech_pipeline", description="Ultra-Maximal Alzheimer's Speech Analysis Pipeline")
    subparsers = parser.add_subparsers(dest="command", required=True, help="Subcommand to run")

    # run_pipeline subcommand (orchestrates full pipeline)
    p_run = subparsers.add_parser("run_pipeline", help="Run full pipeline (build -> matrices -> train -> evaluate)")
    add_run_pipeline_args(p_run)

    # build_features subcommand
    p_build = subparsers.add_parser("build_features", help="Extract features and save .npz feature files")
    p_build.add_argument('--data_dir', required=True)
    p_build.add_argument('--work_dir', required=True)
    p_build.add_argument('--max_frames', type=int, default=DEFAULT_MAX_FRAMES)
    p_build.add_argument('--augment', action='store_true')
    p_build.add_argument('--use_vad', action='store_true')
    p_build.add_argument('--diarize', action='store_true')
    p_build.add_argument('--diarizer_token', type=str, default=None)
    p_build.add_argument('--instructor_label', type=str, default=None)

    # build_matrices subcommand
    p_mats = subparsers.add_parser("build_matrices", help="Fuse feature .npz files into X_seq, X_agg, y")
    p_mats.add_argument('--feature_dir', required=True, help="Directory with .npz feature files (features/)")
    p_mats.add_argument('--out_dir', required=True, help="Output directory to write matrices")
    p_mats.add_argument('--max_seq_len', type=int, default=800)

    # train_sequence subcommand
    p_train_seq = subparsers.add_parser("train_sequence", help="Train deep sequence models on matrices")
    p_train_seq.add_argument('--matrices_dir', required=True)
    p_train_seq.add_argument('--models_dir', required=True)
    p_train_seq.add_argument('--epochs', type=int, default=40)
    p_train_seq.add_argument('--batch_size', type=int, default=16)
    p_train_seq.add_argument('--mixed_precision', action='store_true')

    # train_classical subcommand
    p_train_cls = subparsers.add_parser("train_classical", help="Train classical ML models on aggregated features")
    p_train_cls.add_argument('--matrices_dir', required=True)
    p_train_cls.add_argument('--models_dir', required=True)

    # evaluate subcommand
    p_eval = subparsers.add_parser("evaluate", help="Evaluate saved models on matrices")
    p_eval.add_argument('--models_dir', required=True)
    p_eval.add_argument('--matrices_dir', required=True)
    p_eval.add_argument('--out_dir', required=True)

    # serve subcommand
    p_serve = subparsers.add_parser("serve", help="Start local Flask inference server")
    p_serve.add_argument('--model_type', choices=['classical','sequence'], required=True)
    p_serve.add_argument('--model_fp', required=True)
    p_serve.add_argument('--scaler_fp', type=str, default=None)
    p_serve.add_argument('--port', type=int, default=5000)

    # infer one audio file
    p_infer = subparsers.add_parser("infer", help="Run single-file inference locally")
    p_infer.add_argument('--audio', required=True)
    p_infer.add_argument('--model_type', choices=['classical','sequence'], required=True)
    p_infer.add_argument('--model_fp', required=True)
    p_infer.add_argument('--scaler_fp', type=str, default=None)
    p_infer.add_argument('--max_seq_len', type=int, default=800)

    return parser

def main_cli(argv=None):
    if argv is None:
        argv = sys.argv[1:]
    parser = create_arg_parser()
    args = parser.parse_args(argv)

    if args.command == "run_pipeline":
        run_pipeline_cli(args)
    elif args.command == "build_features":
        build_dataset(args.data_dir, args.work_dir, limit_frames=args.max_frames, augment=args.augment, use_vad=args.use_vad, diarize=args.diarize, diarizer_token=args.diarizer_token, instructor_label=args.instructor_label)
    elif args.command == "build_matrices":
        build_feature_matrices(args.feature_dir, args.out_dir, max_seq_len=args.max_seq_len)
    elif args.command == "train_sequence":
        # load matrices and train all sequence models
        X_seq, X_agg, y = load_feature_matrices(args.matrices_dir)
        run_all_sequence_models(X_seq, y, out_dir=args.models_dir, epochs=args.epochs, batch_size=args.batch_size, mixed_precision=args.mixed_precision)
    elif args.command == "train_classical":
        X_seq, X_agg, y = load_feature_matrices(args.matrices_dir)
        # train/test split
        from sklearn.model_selection import train_test_split
        idx_train, idx_test = train_test_split(np.arange(len(y)), test_size=0.2, stratify=y, random_state=42)
        X_train = X_agg[idx_train]; X_test = X_agg[idx_test]
        y_train = y[idx_train]; y_test = y[idx_test]
        featnames = json.load(open(os.path.join(args.matrices_dir, "feature_names.json")))["feature_names"]
        run_classical_training(X_train, y_train, X_test, y_test, featnames, out_dir=args.models_dir)
    elif args.command == "evaluate":
        evaluate_all_models(args.models_dir, args.matrices_dir, args.out_dir)
    elif args.command == "serve":
        start_flask_server(model_type=args.model_type, model_fp=args.model_fp, scaler_path=args.scaler_fp, port=args.port)
    elif args.command == "infer":
        scaler = None
        if args.scaler_fp:
            try:
                scaler = FeatureScaler.load(args.scaler_fp)
            except Exception:
                scaler = None
        res = inference_single_audio(args.audio, model_type=args.model_type, model_fp=args.model_fp, scaler=scaler, max_seq_len=args.max_seq_len, return_raw=False)
        print(json.dumps(res, indent=2))
    else:
        parser.print_help()

if __name__ == "__main__":
    main_cli()

"""
numpy
scipy
librosa
soundfile
pandas
scikit-learn
matplotlib
seaborn
joblib
tensorflow>=2.10  # or torch if using PyTorch sections
torch               # optional: for PyTorch training portions in PART 10
parselmouth         # optional: for jitter/shimmer (Praat)
gammatone          # optional: GTCC
pyannote.audio      # optional: speaker diarization (heavy)
transformers        # optional: wav2vec2 / BERT embeddings
xgboost             # optional
shap                # optional
flask               # optional: for inference server
"""


